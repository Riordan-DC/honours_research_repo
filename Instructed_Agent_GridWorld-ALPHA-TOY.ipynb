{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructed Agent\n",
    "##### Riordan Callil 2020\n",
    "Topics:\n",
    "- Scene understanding\n",
    "- Machine reading\n",
    "- RL\n",
    "- Question-Answering\n",
    "\n",
    "Thesis: Can an agent learn to retrieve relevant information to explore its environment?\n",
    "Knowledge should not stagnate. Knowledge is meant to be integrated with life. <br>\n",
    "\"[GCN] can successfully learn object-centric navigation policies on semantic maps\"<br>\n",
    "\"[GCN] can successfully learn <b>langauge</b>-centric navigation policies on semantic maps\"<br>\n",
    "\n",
    "Method:\n",
    "Suppose an agent exists within a graph. The graph's nodes are landmarks, connected via edges to poses. The landmarks are represented as words. While the poses represent position. The objective of this agent is to reach an objective. For simplicity, this object is a some position, reachable in the graph. Randomly exploring the graph the agent will find the objective exhaustively. Can it learn to use a set of context sensitive instructions to guide its decisions at each position and reach its objective orders of magnitude faster than random exploration. \n",
    "\n",
    "Reward modelling:\n",
    "An oracle traverses the graph initally and produces the quickest path through the graph. The agent, at every timestep, must select an action. The action is a selection task: Choosing a node *adjacent to the agent to travel. If the node is on the gold path reward + 1. If not, reward 0. \n",
    "\n",
    "\n",
    "<b>Experiment idea:</b> (Should I make the agent forget past nodes? Experiment: Remember visited nodes, it will be harder to find specific instructions if your graph state is large. Maybe attention can help.)<br>\n",
    "<b>Experiment idea (HARDER):</b> Let the agent infer the locations from the items within the room rather than naming the pose nodes.<br>\n",
    "<b>Experiment idea (UNKNOWN):</b> Use multiple words to describe an entity. Then combine the wordvectors when analysing. <br>\n",
    "<b>Experiment idea (HARDER):</b> Use negatives in the instructions to challenge the learning of avoiding policies.<br>\n",
    "<b>Experiment idea (HARDER):</b> The agent has to decide when it reaches the goal by checking the instruction. Like chosing the current node is like saying \"This is where I need to stay\"<br>\n",
    "<b>Model idea (HARDER):</b> Conceptualising sentences as graphs. Annotating graphs with abstractions.<br>\n",
    "<b>Model/Experiment idea (HARDER):</b> Managing state during instruction comprehension. I.e. an instruction changes the environment. How is the action forumlated? How is the state formulated? (Nodes immediate to the agent?)<br>\n",
    "<b>Experiment idea (HARDER):</b>Adding emotive text so the agent learns to develop behaviours based on emotive cues. I.e. The basement is scary. Using a gun is dangerous.<br>\n",
    "<b>Experiment idea (HARDER):</b>As an addition to the above experiment. I would like to see not only attention but inhibition. For example if negative sentiment influenced the decision I want to see that.<br>\n",
    "<b>Experiment idea (HARDER):</b> Tracking entities properties through text and answering questions about change.<br>\n",
    "<b>Experiment idea (EASIER):</b> Formulating the state experiment: I believe the problem is a selection problem. I DONT want a large action space cluttered with specific actions so instead you only can select immediate nodes. Selecting an object picks it up. You can only hold a single item so our agent cant just become a kleptomaniac.<br>\n",
    "<b>Experiment idea (HARDER,GENERALISED)[TODO]:</b> As per the TODO note in the training code. I want to create paths/instructions then build a graph of noise around that valid path. So the challenge is finding and navigating the path and avoiding the noise. <br>\n",
    "<b>Experiment idea (HARDER)[IMPLIMENTED]:</b> Add prev_pose to check if agent is in correct pose. This makes it harder by making the agent have to select its current position when it believes it has found the correct pose. This means random walking is disadvantaged but guided should benefit. <br>\n",
    "<b>Answer</b> Question: Initalise LSTM hidden state; should I use current node as starting state? Answer: No, the hidden state is persistant across sequences and batches. It is the learnt part of the RNN. You can concatonate some extra knowledge or something but not alter the hidden state unless you know what you are doing. <br>\n",
    "<b>Experiment idea (UNKNOWN):</b> Add noise to an environment using nodes related to the nodes on the path. So a fridge node adds the toaster and sink nodes. This might be too much work. \n",
    "\n",
    "\n",
    "<b>Bechmark</b>: Have an agent without instructions. Compare to show how/if the instructions are read. You can also do a random projection to check if there is any learninful learning being performed. Doing a gaussian projection would also be interesting to compare agaisnt a learnt RNN. A projection in this context is doing a matrix multiplication. Instruction vector [3, 300] x [300, 300] (assuming the instruction is a set 3 words long) where the square matrix is the projection matrix. A random projection has random variables. A gaussian one has values from a gaussian distribution which can preserve distance between values.  \n",
    "\n",
    "TODO:\n",
    "- Procedurally generate instructions\n",
    "    - Add observations\n",
    "    - Use templates\n",
    "        - generate_instruction should take a graph, build a path, then from a bunch of \n",
    "- Train and compare to random projection of instructions vs random walker\n",
    "- Investigate RL learning models. Check for the correct implimentation of REINFORCE algorithm\n",
    "\n",
    "BIG QUESTIONS:\n",
    "- Where does an instruction look like and where can we find data on that?\n",
    "- What are we learning about instructions? How to follow them, how to use them to inform our decsisions but what does that mean? Because if I model my environment as text then the thought of the instruction \"Go to the vase\" and the \"vase\" environment node will be close in my mind. The difficulty is how do I learn the nuance of langauge? I need a langauge model which can interprete these objectives correctly. I know I want to find these targets. But if I go back to the original objective: I want to follow the instructions. That means there is a binary outcome: The model followed the instruction or it didnt. So the challenge is to find out WHAT the objective of an instruction is AND then if we followed it or not. So it sort of setting goals for itself. \n",
    "- Perhaps a seperate RNN for learning the world model for predicting the next outcome from following an instruction. The problem with this is that I already exist within a complete, knowable world. I.e. If I have mapped the entire environment then a world model will learn to understand the movement throughout the world with respect to our own actions. But I wonder if this is useful to the internal model. I think it could be if it were responsible which instructions to read, i.e. it could learn that once we have finished instruction 1, we should read instruction 2. But wont most instructions be ordered this way? I could randomize it. But the question remains: Do I need a world model if I can just query a map. The world model can predict my outcomes but how is that useful to following instructions? I mentioned earlier how following instructions is predicting the outcome, perhaps this is the place of the world model?\n",
    "\n",
    "Project Log <Thu,8:45pm,10/9/20>:<br>\n",
    "I have passion for research but I am not putting in the effort. Why? It is because I am not treating it like an art. I am not appreciating how important it is as a challenge. How much I need to learn to collaborate with others and learn from everyone. Ask those much smarter than me. For instance: I think deeply and often about game design. Every choice I obsess over for days, weeks, months, years. However, I rarely let research problems enter the same space in the my mind. I am doing the naive thing and treating art and science like different dialogs of thought. When I was a child I promised myself that I would appreciate the imagination in science. But I have failed. I must recollect my philosophy on research. Develop a philosophy. I know the importance of having a philosophy, not because it must become law, but because it means I have some thought about it at all! This gives my structure, any structure, from which to make these deep descisions. Being sad about my research is common but it is blinding. Sadness is often thought as something that arises from obsessive introspection. But I am not truly looking at myself. I think it is because I am so new to research. (Research, no specific topic, this philosophy is larger than any technology) If I were, at present, I woult realise that I need to ask the RL group members my questions about RL. I would find questions for them after thinking about RL. I need to go to their meetings on monday at 4pm. I am already on S11 on Mondays. Although I dont have a meeting with Niko next Monday, I will be there for tutoring and I WILL remain for the meeting, whether that be online, or in person. I need to write my code. Do my tests. Give myself the time to observe my own data and think deeply about it. I also need to consider the structure of my experiments and what their results mean in success or fail. The real problem is that I am not just breathing, sitting alone, and thinking about the research problem and how the mechanics will work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the text vectors \n",
    "import torchtext\n",
    "\n",
    "\n",
    "fast_text = torchtext.vocab.FastText()\n",
    "node_repr_dim = fast_text.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_adapt_by_suffix', '_check_input_data_sanity', '_check_training_sanity', '_clear_post_train', '_do_train_epoch', '_do_train_job', '_get_job_params', '_get_thread_working_mem', '_job_producer', '_load_specials', '_log_epoch_end', '_log_epoch_progress', '_log_progress', '_log_train_end', '_raw_word_count', '_save_specials', '_set_train_params', '_smart_save', '_train_epoch', '_train_epoch_corpusfile', '_update_job_params', '_worker_loop', '_worker_loop_corpusfile', 'accuracy', 'bucket', 'build_vocab', 'build_vocab_from_freq', 'clear_sims', 'cum_table', 'doesnt_match', 'estimate_memory', 'evaluate_word_pairs', 'hashfxn', 'init_sims', 'iter', 'layer1_size', 'load', 'load_binary_data', 'load_fasttext_format', 'max_n', 'min_count', 'min_n', 'most_similar', 'most_similar_cosmul', 'n_similarity', 'num_ngram_vectors', 'sample', 'save', 'similar_by_vector', 'similar_by_word', 'similarity', 'syn0_lockf', 'syn0_ngrams_lockf', 'syn0_vocab_lockf', 'syn1', 'syn1neg', 'train', 'wmdistance']\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Supervised fastText models are not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3febf6413736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcap_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/riordan/Desktop/.vector_cache/wiki-news-300d-1M.vec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfbkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_facebook_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload_facebook_vectors\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \"\"\"\n\u001b[0;32m-> 1198\u001b[0;31m     \u001b[0mmodel_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_fasttext_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m_load_fasttext_format\u001b[0;34m(model_file, encoding, full_model)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \"\"\"\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fasttext_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m     model = FastText(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fin, encoding, full_model)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mraw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py\u001b[0m in \u001b[0;36m_load_vocab\u001b[0;34m(fin, new_format, encoding)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnlabels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Supervised fastText models are not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s words for fastText model from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Supervised fastText models are not supported"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "cap_path = datapath(\"/Users/riordan/Desktop/.vector_cache/wiki-news-300d-1M.vec\")\n",
    "print(dir(FastText))\n",
    "fbkv = gensim.models.fasttext.load_facebook_vectors(cap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Environment Graph Functions\n",
    "# Niko's GraphMap\n",
    "########\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.spatial.distance\n",
    "from scipy.spatial.distance import pdist\n",
    "import numpy as np\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "# =============================================================================\n",
    "class Node(typing.NamedTuple):\n",
    "    pose : np.ndarray   # we assume 2D poses, i.e. [x,y,theta]\n",
    "    type : str\n",
    "    repr : np.ndarray\n",
    "    id : int = -1     # unique ID for the node independent of its type\n",
    "    name : str = ''   # Label\n",
    "    data : dict = {}  # store additional data in here        \n",
    "    \n",
    "# =============================================================================\n",
    "class Edge(typing.NamedTuple):        \n",
    "    repr : np.ndarray\n",
    "    nodes: tuple      # contains the node IDs, not the names\n",
    "    id : int = -1     # unique ID for the node independent of its type\n",
    "    undirected : bool = True\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "class GraphMap:\n",
    "    def __init__(self, node_repr_dim=64):\n",
    "        self._nodes = {}\n",
    "        self._edges = {}\n",
    "        self._node_counter = 0\n",
    "        self._edge_counter = 0\n",
    "        self._pose_counter = 0\n",
    "        self._affordance_counter =0\n",
    "        self._landmark_counter = 0\n",
    "        self._torch_graph = None\n",
    "        self.node_repr_dim = node_repr_dim      # dimensionality of node representations\n",
    "        \n",
    "        self._pose_mask = None\n",
    "        self._landmark_mask = None\n",
    "        self._affordance_mask = None\n",
    "\n",
    "    # =======================================================================\n",
    "    def _add_node(self, node, connections=[]):\n",
    "        \n",
    "        if type(node.pose) != np.ndarray:\n",
    "            node = node._replace(pose=np.array(node.pose))\n",
    "            \n",
    "        if type(node.repr) != np.ndarray:\n",
    "            node = node._replace(repr=np.array(node.repr))\n",
    "            \n",
    "        self._nodes[self._node_counter] = node\n",
    "        self._node_counter += 1\n",
    "        \n",
    "        return self._node_counter\n",
    "    \n",
    "    # =======================================================================\n",
    "    def node(self, name):\n",
    "        \"\"\"Return a node identified by its name. Returns None if it does not exist.\"\"\"        \n",
    "        if type(name)==str:\n",
    "            return next((n for n in self._nodes.values() if n.name==name), None)\n",
    "        elif type(name==int):            \n",
    "            return self._nodes[name]\n",
    "        else:\n",
    "            raise Exception('Specify the node either by its name (as a string) or its id (as int).')        \n",
    "    \n",
    "    # =======================================================================\n",
    "    def connected(self, node1, node2):\n",
    "        \"\"\"Returns True if two nodes exist and are connected by an edge. Nodes are specified by either their name or id.\"\"\"\n",
    "        \n",
    "        # first get the node IDs\n",
    "        n1 = self.node(node1)\n",
    "        if n1 is not None:\n",
    "            id1 = n1.id\n",
    "        else:\n",
    "            return False\n",
    "                    \n",
    "        n2 = self.node(node2)\n",
    "        if n2 is not None:\n",
    "            id2 = n2.id\n",
    "        else:\n",
    "            return False    \n",
    "       \n",
    "        for e in self._edges.values():            \n",
    "            if e.nodes == (id1, id2):                \n",
    "                return True\n",
    "            if e.nodes == (id2, id1) and e.undirected==True:\n",
    "                return True\n",
    "        return False\n",
    "    # =======================================================================\n",
    "    def remove_node(self, name=None, id=None):\n",
    "        if name is not None and id is not None:\n",
    "            raise Exception('Specify either the name or the id of the node to be deleted, but not both')        \n",
    "        else:\n",
    "            if name is not None:\n",
    "                n = self.node(name)\n",
    "                if n is not None:\n",
    "                    id = n.id                \n",
    "            \n",
    "            # does the node actually exist?\n",
    "            if id is None:\n",
    "                return False\n",
    "            \n",
    "            # remove the node                        \n",
    "            self._nodes.pop(id, None)     \n",
    "            \n",
    "            # remove all edges containing the node\n",
    "            for k in [k for k in self._edges if id in self._edges[k].nodes]:                            \n",
    "                self._edges.pop(k, None)\n",
    "            \n",
    "    # =======================================================================   \n",
    "    def remove_all_edges(self):\n",
    "        self._edges = {}\n",
    "    \n",
    "    # =======================================================================             \n",
    "    def add_pose(self, pose, name=None, repr=None):        \n",
    "        node = Node(pose=pose, \n",
    "                    name=name,\n",
    "                    type='pose',\n",
    "                    repr=np.zeros(self.node_repr_dim) if repr is None else repr,\n",
    "                    id=self._node_counter)\n",
    "        self._pose_counter += 1\n",
    "        self._add_node(node)\n",
    "        return node\n",
    "        \n",
    "    # =======================================================================\n",
    "    def add_landmark(self, pose, name=None, repr=None, data={}):        \n",
    "                \n",
    "        if name is None:\n",
    "            name='l%d' % self._landmark_counter\n",
    "            \n",
    "        # does a landmark with this name exist already?\n",
    "        if self.node(name) is not None:\n",
    "            return None\n",
    "            \n",
    "        node = Node(pose=pose, \n",
    "                    name=name,\n",
    "                    type='landmark',\n",
    "                    repr=np.zeros(self.node_repr_dim) if repr is None else repr,\n",
    "                    id=self._node_counter,\n",
    "                    data=data)\n",
    "        self._landmark_counter += 1\n",
    "        self._add_node(node)\n",
    "        return node\n",
    "    \n",
    "    # =======================================================================\n",
    "    def add_affordance(self, node, name, affordance, repr=None):\n",
    "        pose = self.node(node).pose + np.array([0.1, 0.1, 0])\n",
    "        \n",
    "        affordance_node = Node(pose=pose, \n",
    "                            name=name,\n",
    "                            type='affordance',\n",
    "                            repr=np.zeros(self.node_repr_dim) if repr is None else repr,\n",
    "                            id=self._node_counter)\n",
    "\n",
    "        self._affordance_counter += 1\n",
    "        self._add_node(affordance_node)\n",
    "        \n",
    "        # connect the affordance with its target node        \n",
    "        self.add_edge((node, affordance_node.id))\n",
    "    \n",
    "    # =======================================================================\n",
    "    def add_edge(self, nodes, repr=None, undirected=True):\n",
    "        \"\"\"Add an edge between two nodes. Expect nodes to be a tuple of two names or ids\"\"\"\n",
    "        if type(nodes) != list:\n",
    "            nodes=list(nodes)\n",
    "                   \n",
    "        for i in range(2):\n",
    "            n = self.node(nodes[i])    # get it by its name\n",
    "            if n is None:\n",
    "                raise Exception('Node with name %s not found.' % nodes[i])\n",
    "            else:\n",
    "                nodes[i] = n.id\n",
    "        \n",
    "        e = Edge(id=self._edge_counter, \n",
    "                 nodes=(nodes[0], nodes[1]), \n",
    "                 repr=np.empty(0) if repr is None else repr,\n",
    "                 undirected=undirected\n",
    "                )\n",
    "        \n",
    "        self._edges[self._edge_counter] = e\n",
    "        self._edge_counter += 1\n",
    "    \n",
    "    # =======================================================================\n",
    "    def add_nearest_neighbor_edges(self, thresh=1.0):\n",
    "        poses = np.array([n.pose for n in self._nodes.values() if n.type=='pose'])\n",
    "        ids = [n.id for n in self._nodes.values() if n.type=='pose']\n",
    "        \n",
    "        # use only the first two components of the pose, which is [x.y.theta]        \n",
    "        poses = poses[:,0:2]\n",
    "        \n",
    "        # get the pairwise distance between all poses\n",
    "        D = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(poses))\n",
    "        \n",
    "        # prevent connecting the nodes with itself by setting the self-distance to greater than the threshold\n",
    "        mask = np.triu_indices_from(D)\n",
    "        D[mask] = thresh+1\n",
    "        \n",
    "        # mask the pairs that are closer than the threshold\n",
    "        d=np.nonzero(D<=thresh)\n",
    "               \n",
    "        for i in range(len(d[0])):        \n",
    "            n1 = ids[d[0][i]]\n",
    "            n2 = ids[d[1][i]]\n",
    "            self.add_edge((n1,n2))                    \n",
    "\n",
    "    # =======================================================================\n",
    "    def draw(self, draw_edges=True):\n",
    "        # draw pose nodes\n",
    "        loc = np.array([x.pose for x in self._nodes.values() if x.type == 'pose'])    \n",
    "        if loc.size>0:\n",
    "            plt.scatter(loc[:,0], loc[:,1])\n",
    "        \n",
    "        # draw landmark nodes\n",
    "        loc = np.array([x.pose for x in self._nodes.values() if x.type == 'landmark'])    \n",
    "        if loc.size>0:\n",
    "            plt.scatter(loc[:,0], loc[:,1])\n",
    "            \n",
    "        # draw affordance nodes\n",
    "        loc = np.array([x.pose for x in self._nodes.values() if x.type == 'affordance'])    \n",
    "        if loc.size>0:\n",
    "            plt.scatter(loc[:,0], loc[:,1], marker='s')\n",
    "        \n",
    "        # Annotate\n",
    "        for node in self._nodes.values():\n",
    "            plt.text(node.pose[0], node.pose[1] + 0.1, 'UNK' if node.name == None else node.name)\n",
    "        \n",
    "        # draw edges\n",
    "        if draw_edges:\n",
    "            poses = [self._nodes[id].pose for e in self._edges.values() for id in e.nodes]        \n",
    "            x = []; y = []\n",
    "            for i in range(0, len(poses), 2):\n",
    "                x.append([poses[i][0], poses[i+1][0]])\n",
    "                y.append([poses[i][1], poses[i+1][1]])                \n",
    "            plt.plot(np.array(x).T, np.array(y).T, 'b', linewidth=0.1)\n",
    "    \n",
    "    # =======================================================================\n",
    "    def construct_torch_graph(self):\n",
    "        \n",
    "        # gather all the node representations\n",
    "        node_repr = np.stack([n.repr for n in self._nodes.values()])\n",
    "       \n",
    "        # we might need a few masks, e.g. which nodes are poses, which are landmarks, which actions \n",
    "        node_type = np.array([n.type for n in self._nodes.values()])\n",
    "        self._pose_mask = node_type == 'pose'\n",
    "        self._landmark_mask = node_type == 'landmark'   \n",
    "        self._affordance_mask = node_type == 'affordance'               \n",
    "                \n",
    "        # get the edge structure\n",
    "        edge_index = []\n",
    "        for edge in self._edges.values():\n",
    "            edge_index.append([edge.nodes[0], edge.nodes[1]])\n",
    "            if edge.undirected:\n",
    "                edge_index.append([edge.nodes[1], edge.nodes[0]])\n",
    "              \n",
    "        # build the graph        \n",
    "        self._torch_graph = torch_geometric.data.Data(x=torch.tensor(node_repr, dtype=torch.float), \n",
    "                                                      edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "                                                    )\n",
    "    # =======================================================================\n",
    "    def pose_mask(self):\n",
    "        return self._pose_mask\n",
    "    \n",
    "    # =======================================================================\n",
    "    def landmark_mask(self):\n",
    "        return self._landmark_mask\n",
    "             \n",
    "    # =======================================================================\n",
    "    def affordance_mask(self):\n",
    "        return self._affordance_mask\n",
    "    \n",
    "    # =======================================================================\n",
    "    def connected_landmarks(self, node):\n",
    "        \"\"\"Returns the node IDs of all landmarks connected to the specified node.\"\"\"\n",
    "        n = self.node(node)\n",
    "        if n is None:            \n",
    "            return []        \n",
    "        else:\n",
    "            result = [e.nodes[1] for e in self._edges.values() if e.nodes[0]==n.id and self._nodes[e.nodes[1]].type=='landmark']\n",
    "            result += [e.nodes[0] for e in self._edges.values() if e.nodes[1]==n.id and e.undirected==True and self._nodes[e.nodes[0]].type=='landmark']\n",
    "            return result\n",
    "\n",
    "    # =======================================================================\n",
    "    def connected_to_type(self, node, object_type):\n",
    "        \"\"\"Returns True if a node is connected to a landmark with a certain objectType.\"\"\"\n",
    "        if object_type.lower() in self.connected_landmark_types(node):\n",
    "            return True\n",
    "        else:\n",
    "            return False        \n",
    "\n",
    "    # =======================================================================\n",
    "    def connected_landmark_types(self, node):\n",
    "        return [self.node(obj).data['objectType'].lower() for obj in graph.connected_landmarks(node)]\n",
    "\n",
    "    # =======================================================================\n",
    "    def landmark_types(self):\n",
    "        return [n.data['objectType'].lower() for n in self._nodes.values() if n.type=='landmark']\n",
    "    \n",
    "    # =======================================================================\n",
    "    def adjacent_nodes(self, node):\n",
    "        ''' Return all nodes connected to node. Node must be a node id'''\n",
    "        return [n for n in self._nodes.values() if self.connected(n.id, node)]\n",
    "    # =======================================================================\n",
    "    def subgraph(self, node):\n",
    "        ''' Construct and return a torch graph of a specific neighbourhood around a node.\n",
    "            NOTE: This is useless for my task. Using a GCN on a neighbourhood is overkill.\n",
    "            The GCN wants to learn the structure of the graph and perhaps a small graph\n",
    "            isnt going to be too useful. \n",
    "        '''\n",
    "        subgraph = GraphMap()\n",
    "        \n",
    "        # Find neighbouring nodes\n",
    "        nodes = [self.node(node)]\n",
    "        nodes.extend(self.adjacent_nodes(node))\n",
    "        \n",
    "        ids = torch.tensor([n.id for n in nodes])\n",
    "        \n",
    "        for node in nodes:\n",
    "            if node.type == 'pose':\n",
    "                subgraph.add_pose(node.pose, node.name, node.repr)\n",
    "            elif node.type == 'landmark':\n",
    "                subgraph.add_landmark(node.pose, node.name, node.repr)\n",
    "       \n",
    "        # get the edge structure\n",
    "        for node in range(1, len(nodes), 1):\n",
    "            subgraph.add_edge([0, node])\n",
    "            subgraph.add_edge([node, 0])\n",
    "              \n",
    "        # build the graph        \n",
    "        return subgraph, ids\n",
    "    \n",
    "    def subgraph_mask(self, node):\n",
    "        '''\n",
    "        Get a subgraph consisting of adjacent nodes to \"node\" as a mask over the original graph.\n",
    "        '''\n",
    "        # Get ids of all neighbours\n",
    "        neighbours = torch.tensor([node])\n",
    "        neighbours = torch.cat((neighbours, torch.tensor([n.id for n in self.adjacent_nodes(node)])), dim=0)\n",
    "        mask = torch.zeros(self._node_counter, dtype=torch.bool)\n",
    "        mask[neighbours] = True\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# AI2THOR Exploration\n",
    "####\n",
    "import ai2thor.controller\n",
    "import numpy as np\n",
    "\n",
    "def explore_environment(environment_name='FloorPlan29'):\n",
    "    controller = ai2thor.controller.Controller()\n",
    "    controller.start()\n",
    "\n",
    "    # can be any one of the scenes FloorPlan###\n",
    "    controller.reset(environment_name)\n",
    "    controller.step(dict(action='Initialize', gridSize=0.25))\n",
    "    event=controller.step(dict(action = 'InitialRandomSpawn', randomSeed = 0,minFreePerReceptacleType = [{'objectType': 'CounterTop', 'count': 1}], numRepeats = [{'objectType': 'Tomato', 'count': 20}]))\n",
    "\n",
    "    # get all valid positions\n",
    "    event = controller.step(dict(action='GetReachablePositions'))\n",
    "    reachable = event.metadata['reachablePositions']\n",
    "\n",
    "    # this is our graph map\n",
    "    graph = GraphMap(node_repr_dim=node_repr_dim)\n",
    "\n",
    "    # teleport the robot to all positions, then rotate in place by a fixed angle and remember which objects are visible\n",
    "    n = 0\n",
    "    for loc in reachable:\n",
    "        action = dict(action='Teleport')\n",
    "        action.update(loc)    \n",
    "        controller.step(action)\n",
    "\n",
    "        for rot in range(0,360,45):   # rotate in 30 deg increments\n",
    "            event = controller.step(dict(action='Rotate', rotation=rot))\n",
    "            current_pose = graph.add_pose(pose=[loc['x'], loc['z'], rot], name=f\"Pose{n}\")\n",
    "            n += 1\n",
    "\n",
    "            # add objects and connect them to the pose\n",
    "            for obj in [o for o in event.metadata['objects'] if o['visible']]:\n",
    "                name = obj['objectId']\n",
    "                pose = [obj['position']['x'], obj['position']['z'], 0]      \n",
    "                repr = fast_text.get_vecs_by_tokens(obj['objectType'].lower())\n",
    "                graph.add_landmark(pose=pose, name=name, data=obj, repr=repr)\n",
    "                graph.add_edge((current_pose.name, name), undirected=True)\n",
    "    \n",
    "    print(\"Explore: Impliment pose node names\")\n",
    "\n",
    "    # make sure neighboring poses are connected (0.25 is the default grid size of the simulator)\n",
    "    graph.add_nearest_neighbor_edges(0.25*np.sqrt(2))            \n",
    "#     graph.draw(draw_edges=False)\n",
    "\n",
    "    # build the torch_geometric graph structure\n",
    "    graph.construct_torch_graph()\n",
    "        \n",
    "    return graph, controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Instructions and entities\n",
    "####\n",
    "# Path, pose, entities, and instruction set\n",
    "path = [\n",
    "        {\n",
    "            \"node_name\": \"bedroom\",\n",
    "            \"pose\" : [0,0,0],\n",
    "            \"instruction\" : \"move towards the hallway\",\n",
    "            \"instruction_vector\": None,\n",
    "            \"entities\": [\"bed\", \"lamp\", \"chair\"]\n",
    "        }, \n",
    "        {\n",
    "            \"node_name\": \"hallway\",\n",
    "            \"pose\" : [0.5,0.5,0],\n",
    "            \"instruction\" : \"to the kitchen\",\n",
    "            \"instruction_vector\": None,\n",
    "            \"entities\" : [\"painting\", \"rug\", \"window\"]\n",
    "        }, \n",
    "        {\n",
    "            \"node_name\": \"kitchen\",\n",
    "            \"pose\" : [1,1,0],\n",
    "            \"instruction\" : \"traverse to the garage\",\n",
    "            \"instruction_vector\": None,\n",
    "            \"entities\" : [\"toaster\", \"microwave\", \"teapot\"]\n",
    "        }, \n",
    "        {\n",
    "            \"node_name\": \"garage\",\n",
    "            \"pose\" : [1,2,0],\n",
    "            \"instruction\" : \"\",\n",
    "            \"instruction_vector\": None,\n",
    "            \"entities\" : [\"car\", \"bicycle\", \"wheel\"]\n",
    "        }\n",
    "]\n",
    "\n",
    "REMOVE_INSTRUCTIONS = False\n",
    "def add_instruction_vectors(path):\n",
    "    # Add instruction vectors\n",
    "    for instruction in path:\n",
    "        word_array = instruction[\"instruction\"].split()\n",
    "        word_cnt = len(word_array)\n",
    "        if REMOVE_INSTRUCTIONS: # Empty test instructions\n",
    "            instruction[\"instruction_vector\"] = torch.zeros(1, node_repr_dim)\n",
    "        else:\n",
    "            if word_cnt == 0:\n",
    "                instruction[\"instruction_vector\"] = torch.zeros(1, node_repr_dim)\n",
    "            else:\n",
    "                instruction[\"instruction_vector\"] = torch.zeros(word_cnt, node_repr_dim)\n",
    "                for word_idx in range(word_cnt):\n",
    "                    instruction[\"instruction_vector\"][word_idx] = fast_text.get_vecs_by_tokens(word_array[word_idx].lower())\n",
    "\n",
    "add_instruction_vectors(path)\n",
    "\n",
    "# Locations & Entities\n",
    "path_locations = [\"kitchen\", \"bedroom\", \"bathroom\", \"toilet\", \"garage\", \n",
    "                  \"basement\", \"office\", \"lobby\", \"hallway\", \"garden\", \n",
    "                  \"office\", \"lobby\", \"basement\", \"bathroom\", \"fridge\", \n",
    "                  \"closet\", \"shower\", \"pantry\", \"staircase\", \"door\",\n",
    "                  \"gate\", \"lawn\", \"path\", \"tree\"\n",
    "                 ]\n",
    "path_entities = [\"toaster\",\"lamp\",\"laptop\",\"couch\",\"television\",\"microwave\",\"table\",\"plate\",\"cup\",\"stove\",\"teapot\",\"chair\",\"car\",\"flowers\",\"towel\",\"bed\",\"painting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gate] -> \"pantry\"\n",
      "[pantry] -> \"door\"\n",
      "[door] -> \"pantry\"\n",
      "[pantry] -> \"shower\"\n",
      "[shower] -> \"\"\n",
      "------------------------------\n",
      "[garden] -> \"basement\"\n",
      "[basement] -> \"door\"\n",
      "[door] -> \"closet\"\n",
      "[closet] -> \"toilet\"\n",
      "[toilet] -> \"\"\n",
      "------------------------------\n",
      "[lobby] -> \"shower\"\n",
      "[shower] -> \"pantry\"\n",
      "[pantry] -> \"door\"\n",
      "[door] -> \"gate\"\n",
      "[gate] -> \"\"\n",
      "------------------------------\n",
      "[closet] -> \"toilet\"\n",
      "[toilet] -> \"closet\"\n",
      "[closet] -> \"shower\"\n",
      "[shower] -> \"gate\"\n",
      "[gate] -> \"\"\n",
      "------------------------------\n",
      "[office] -> \"gate\"\n",
      "[gate] -> \"shower\"\n",
      "[shower] -> \"gate\"\n",
      "[gate] -> \"pantry\"\n",
      "[pantry] -> \"\"\n",
      "------------------------------\n",
      "[gate] -> \"pantry\"\n",
      "[pantry] -> \"shower\"\n",
      "[shower] -> \"door\"\n",
      "[door] -> \"closet\"\n",
      "[closet] -> \"\"\n",
      "------------------------------\n",
      "[garden] -> \"basement\"\n",
      "[basement] -> \"gate\"\n",
      "[gate] -> \"shower\"\n",
      "[shower] -> \"pantry\"\n",
      "[pantry] -> \"\"\n",
      "------------------------------\n",
      "[garden] -> \"closet\"\n",
      "[closet] -> \"toilet\"\n",
      "[toilet] -> \"closet\"\n",
      "[closet] -> \"office\"\n",
      "[office] -> \"\"\n",
      "------------------------------\n",
      "[shower] -> \"pantry\"\n",
      "[pantry] -> \"door\"\n",
      "[door] -> \"pantry\"\n",
      "[pantry] -> \"path\"\n",
      "[path] -> \"\"\n",
      "------------------------------\n",
      "[gate] -> \"shower\"\n",
      "[shower] -> \"closet\"\n",
      "[closet] -> \"office\"\n",
      "[office] -> \"gate\"\n",
      "[gate] -> \"\"\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### # Home Environment\n",
    "######\n",
    "\"\"\"\n",
    "Procedural Data\n",
    "- Procedurally generated environments\n",
    "- Procedurally generated path\n",
    "Method:\n",
    "Randomly, or manually, make an envirnonment. Manual environments are good for representing a real environmnet.\n",
    "Then randomly walk though the environment graph, making sure not to create loops or double backs. \n",
    "Every step it explores, collect all the landmarks and the name of the node visited. (Much like niko's change to \n",
    "the affordance graph I will not be adding blank pose nodes. Only named pose nodes. In the exploring agent this\n",
    "means it will only add nodes to its path it A: has not visited, B: has a name)\n",
    "Once all the nodes and landmarks have been collected along the path.\n",
    "For each node in the path create an instruction with templates.\n",
    "E.g: <move action>.<observation que>\n",
    "\n",
    "\"\"\"\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch_geometric\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "home = GraphMap(node_repr_dim=64)\n",
    "\n",
    "# Add landmark basic structure\n",
    "for x in np.linspace(0, 5, 5):\n",
    "    for y in np.linspace(0, 5, 5):\n",
    "        name = random.choice(path_locations)\n",
    "        home.add_pose([x, y, 0], name=name, repr=fast_text.get_vecs_by_tokens(name.lower()))\n",
    "\n",
    "#home.add_nearest_neighbor_edges(2.0) # HARDER, Avg graph degree larger\n",
    "home.add_nearest_neighbor_edges(1.5)\n",
    "\n",
    "def generate_path(graph, path_length):\n",
    "    ############\n",
    "    # TODO: Use networkx to find all paths. This is dumb.\n",
    "    ############\n",
    "    start_id, start_node = random.choice(list(graph._nodes.items()))\n",
    "    path = [start_node]\n",
    "    current_node = start_id\n",
    "    visited_nodes = [start_id]\n",
    "    for step in range(path_length):\n",
    "        adj_nodes = graph.adjacent_nodes(current_node)\n",
    "        if len(adj_nodes) == 0:\n",
    "            break\n",
    "        next_node = random.choice(adj_nodes)\n",
    "        while next_node.id in visited_nodes:\n",
    "            if len(adj_nodes) == 0:\n",
    "                print(\"--- Failed to build complete path without doubling back. ---\")\n",
    "                return -1\n",
    "            else:\n",
    "                next_node = adj_nodes.pop(random.randrange(0, len(adj_nodes)))\n",
    "        visited_nodes.append(next_node.id)\n",
    "        path.append(next_node)\n",
    "        current_node = next_node.id\n",
    "        \n",
    "    return path\n",
    "\n",
    "def generate_instructions(graph, path_length, instruction_actions, instruction_observations=None):\n",
    "    \"\"\"\n",
    "    Given a graph, path, and set of templates for instructions and observations, this function\n",
    "    will find a path in the graph and assign instructions at each node to visit the next.\n",
    "    The path_length variable is the desired path length, however, the generate_path method does not\n",
    "    double back so you can potentially have a break when the path cannot be generated. A brute force\n",
    "    approach might be to research if it isnt path_length but I think nextworkx could be used to give us\n",
    "    paths of a certain length. If instruction_actions or instruction_observations arent provided a default\n",
    "    instruction of \"move to [next_node]\" will be generated.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    path = generate_path(graph, path_length)\n",
    "    while path == -1:\n",
    "        path = generate_path(graph, path_length)\n",
    "        print(\"TRYING AGAIN\")\n",
    "    \n",
    "    instructions = []\n",
    "    path_length = len(path)\n",
    "    \n",
    "    for node_idx in range(len(path)):\n",
    "        node = path[node_idx]\n",
    "        local_goal = path[node_idx+1].name if node_idx != path_length-1 else \"\"\n",
    "        move_action = \"\" if local_goal == \"\" else f\"{local_goal}\" #f\"move to { local_goal }\" # TEMP CHANGE TEST\n",
    "        instruction = {\n",
    "            \"node_name\": node.name,\n",
    "            \"node_id\" : node.id,\n",
    "            \"pose\": node.pose,\n",
    "            \"instruction\": move_action,\n",
    "            \"instruction_vector\": None,\n",
    "            \"entities\": graph.adjacent_nodes(node.id)\n",
    "        }\n",
    "        instructions.append(instruction)\n",
    "    add_instruction_vectors(instructions)\n",
    "    return instructions\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    p2 = generate_instructions(home, 4, None, None)\n",
    "    for n in p2:\n",
    "        print(f\"[{n['node_name']}] -> \\\"{n['instruction']}\\\"\")\n",
    "    print('-' * 30)\n",
    "\n",
    "home.draw()\n",
    "plt.figure()\n",
    "sg, ids = home.subgraph(0)\n",
    "sg.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Models\n",
    "####\n",
    "\n",
    "# Dual Model (Seperate backprop)\n",
    "# GCN(Graph) -> Embedded Graph -> Dense(Embedded Graph) -> Instruction (Selection)\n",
    "# RNN(Embedded Graph, Instruction) -> Action (Selection)  \n",
    "\n",
    "# MonoModel (Fully learnable)\n",
    "# GCN(Graph) -> Embedded Graph -> RNN(Instruction/s, Embedded Graph) -> Action (Selection)\n",
    "\n",
    "# *RNN = BiLSTM from DrQA / Standford Attentative Reader\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import torch.nn as nn\n",
    "\n",
    "class GraphEmbedding(torch.nn.Module):\n",
    "    def __init__(self, _node_repr_dim):\n",
    "        super(GraphEmbedding, self).__init__()\n",
    "        self.conv1 = GCNConv(_node_repr_dim, _node_repr_dim)\n",
    "        \n",
    "    def forward(self, data):             \n",
    "        x, edge_index = data.x, data.edge_index            \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class RNNInstructor(torch.nn.Module):\n",
    "    def __init__(self, _node_repr_dim):\n",
    "        super(RNNInstructor, self).__init__()\n",
    "        # RNN(Embedded Graph, Instruction) -> Action (Selection)  \n",
    "        #self.graph_embedding = GraphEmbedding(_node_repr_dim)\n",
    "        #self.rnn = nn.LSTM(_node_repr_dim, _node_repr_dim)\n",
    "        self.fc1 = nn.Linear(_node_repr_dim * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, graph_data, instruction):     \n",
    "        '''\n",
    "        graph_data (torch.tensor(number_of_nodes, graph_node_embedding_dimension)): Graph node embeddings\n",
    "        instruction (torch.tensor(number_of_words, word_vector_size)): Word vector embeddings\n",
    "        \n",
    "        What this model is doing is something i might relate to sequence alignment. But instead of two\n",
    "        sequences I have a sequence of words and a set of nodes. \n",
    "        Using attention to find the strongest connections between a member of the set and a member of a sequence.\n",
    "        \n",
    "        @return: Attention vector along number of nodes\n",
    "        '''\n",
    "        \n",
    "        # graph_repr = self.graph_embedding(graph_data)\n",
    "        graph_repr = graph_data.x\n",
    "        \n",
    "        instruction = instruction.unsqueeze(1)\n",
    "        #output, _ = self.rnn(instruction)\n",
    "        #final_output = output[-1] # Take last state of the LSTM\n",
    "        \n",
    "        target = instruction[0] # Skip LSTM, take target word\n",
    "                \n",
    "        # TODO: Understand why niko does this, i think it is because graph embedding learns to embed works with their neighbours\n",
    "        # His word embeddings are dim 300 but his graph node embeddings are dim 64. So the following\n",
    "        # scales converts his embedding to a graph node embedding\n",
    "        #target = torch.matmul(final_output, self.graph_embedding.conv1.weight)    # use same weights for target and node representations x\n",
    "        #target = F.relu(target)\n",
    "        \n",
    "        # Concatenate representations for instruction embedding and the node representations\n",
    "        concatenated = torch.cat((graph_repr, target.squeeze(0).expand(graph_repr.shape[0], -1)), dim=1) \n",
    "        #print(f\"Concatenated shape -> {concatenated.shape}\")\n",
    "        x = self.fc1(concatenated)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # Outputs interpreted as q-values, do NOT relu these values!\n",
    "        return x\n",
    "\n",
    "class RandomInstructor(torch.nn.Module):\n",
    "    def __init__(self, _node_repr_dim):\n",
    "        super(RandomInstructor, self).__init__()\n",
    "        self.z = torch.nn.parameter.Parameter()\n",
    "        \n",
    "    def forward(self, graph_data, instruction):     \n",
    "        '''\n",
    "        graph_data (torch.tensor(number_of_nodes, graph_node_embedding_dimension)): Graph node embeddings\n",
    "        instruction (torch.tensor(number_of_words, word_vector_size)): Word vector embeddings\n",
    "        \n",
    "        Random walker\n",
    "        @return: Attention vector along number of nodes\n",
    "        '''\n",
    "        x, edge_index = graph_data.x, graph_data.edge_index            \n",
    "        return torch.ones(x.shape[0], 1)\n",
    "\n",
    "# Model Dictionary\n",
    "InstructorModels = {\n",
    "    \"RNNInstructor\" : RNNInstructor,\n",
    "    \"RandomInstructor\" : RandomInstructor,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING STARTS: 2020-10-09 07:56:04.798326\n",
      "CUDA available? False\n",
      "----------------------------------------\n",
      "MODEL NAME:RNNInstructor\n",
      "EXPERIMENT_NAME:INSTRUCTIONS\n",
      "LEARNER:True\n",
      "ENVS:[('Home', <__main__.GraphMap object at 0x190323ed0>)]\n",
      "EPISODES:1000\n",
      "MAX TIMESTEPS:3\n",
      "----------------------------------------\n",
      "Session 0     Run 0\n",
      "Episode: 100\tLast Reward: 3.0\tAverage Reward(-1 - 1): -0.0165016501650165\n",
      "Episode: 200\tLast Reward: 3.0\tAverage Reward(-1 - 1): 0.9933333333333333\n",
      "SAVING BEST AGENT: 2020-10-09 07:56:06.902750\n",
      "Episode: 300\tLast Reward: 3.0\tAverage Reward(-1 - 1): 1.0\n",
      "SAVING BEST AGENT: 2020-10-09 07:56:07.896109\n",
      "Episode: 400\tLast Reward: 3.0\tAverage Reward(-1 - 1): 1.0\n",
      "Episode: 500\tLast Reward: 3.0\tAverage Reward(-1 - 1): 0.9933333333333333\n",
      "Episode: 600\tLast Reward: 3.0\tAverage Reward(-1 - 1): 1.0\n",
      "Episode: 700\tLast Reward: 3.0\tAverage Reward(-1 - 1): 1.0\n",
      "Episode: 800\tLast Reward: 3.0\tAverage Reward(-1 - 1): 0.9933333333333333\n",
      "Episode: 900\tLast Reward: 3.0\tAverage Reward(-1 - 1): 1.0\n",
      "--------------------------------------------------\n",
      "FINISHED\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####\n",
    "# Training\n",
    "####\n",
    "import pandas as pd\n",
    "import json, pickle, time\n",
    "import tqdm\n",
    "import networkx\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return np.array(returns)\n",
    "\n",
    "\n",
    "print(f\"TRAINING STARTS: {datetime.now()}\")\n",
    "np.random.seed(42)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"CUDA available? %s\" % torch.cuda.is_available())\n",
    "\n",
    "dataframe = pd.DataFrame(columns=[\"session\", \"run\", \"environment\", \"policy\", \"mode\", \"episode\", \"reward\", \"time\", \"success\"])\n",
    "session = 0\n",
    "\n",
    "model_name = \"RNNInstructor\"\n",
    "experiment_name = \"RANDOM\" if model_name == \"RandomInstructor\" else \"INSTRUCTIONS\" if REMOVE_INSTRUCTIONS == False else \"NO_INSTRUCTIONS\"\n",
    "learner = False if model_name == \"RandomInstructor\" else True\n",
    "environments = [(\"Home\", home)]\n",
    "episodes = 1000 #50000\n",
    "max_timesteps = 3\n",
    "path_length = 3\n",
    "runs = 1\n",
    "lr = 0.001\n",
    "TRAIN_DEBUG = True\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"MODEL NAME:{model_name}\\nEXPERIMENT_NAME:{experiment_name}\\nLEARNER:{learner}\\nENVS:{environments}\\nEPISODES:{episodes}\\nMAX TIMESTEPS:{max_timesteps}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for env in environments:\n",
    "    # To stop exploring the AI2THOR environment every time I train\n",
    "    # TODO: New graph building function. Form: F(path) -> Graph which includes path + random noise paths\n",
    "    # The above todo will help us prevent overfitting to a single graph. More paths will be required to learn generalised paths/instructions\n",
    "    if type(env) is str:\n",
    "        environment_name = env\n",
    "        graph_path = \"%s.envg\" % (env) # if type(env) is str else f\"custom-graph-N{env._node_counter}E{env._edge_counter}P{env._pose_counter}L{env._landmark_counter}\")\n",
    "        if os.path.exists(graph_path):\n",
    "            # Load pickle\n",
    "            print(\"Loading graph: \", graph_path)\n",
    "            graph = pickle.load(open(graph_path, 'rb'))\n",
    "        else:\n",
    "            print(\"Building graph: \", graph_path)\n",
    "            graph, controller = explore_environment(env)\n",
    "            pickle.dump(graph, open(graph_path, 'wb'))\n",
    "            controller.stop()\n",
    "    else:\n",
    "        graph = env[1]\n",
    "        environment_name = env[0]\n",
    "\n",
    "    graph.draw()\n",
    "\n",
    "    # A model is created each run\n",
    "    # A positive reward is provided if in 10 episodes it finds a target \n",
    "    for run in range(runs): \n",
    "        print('Session %d     Run %d' % (session, run))\n",
    "        \n",
    "        # initialise the network\n",
    "        model = InstructorModels[model_name](node_repr_dim).to(device) #Instructor(node_repr_dim).to(device)\n",
    "        model.train()\n",
    "        \n",
    "        # set up the optimiser and the rest of the training pipeline\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        reward = []\n",
    "        used_timesteps = []\n",
    "        success = []\n",
    "        num_successes = 0\n",
    "        current_pose = None\n",
    "        best_avg_r = 0\n",
    "        run_avg_distance = []\n",
    "        \n",
    "        # BIG TEST, Seeing if this thing can learn at all ??\n",
    "        path = generate_instructions(graph, path_length, None, None)\n",
    "        poses = [k[\"node_name\"] for k in path]\n",
    "                    \n",
    "        # train for a number of episodes\n",
    "        for episode_num in range(episodes): #tqdm.notebook.tqdm(range(episodes)):\n",
    "            # Generate instructions\n",
    "            #path = generate_instructions(graph, path_length, None, None)\n",
    "            #poses = [k[\"node_name\"] for k in path]\n",
    "            \n",
    "            # DEBUG\n",
    "            if False and TRAIN_DEBUG:\n",
    "                print(\">>> PATH <<<\")\n",
    "                for n in path:\n",
    "                    print(n['node_name'], n['node_id'])\n",
    "            \n",
    "            # Choose a starting pose\n",
    "            first_pose = path[0]['node_id']\n",
    "            current_pose = first_pose\n",
    "            # Choose a target pose\n",
    "            target_pose = path[-1]['node_id']\n",
    "            \n",
    "            # DEBUG\n",
    "            if False and TRAIN_DEBUG:\n",
    "                print(f\"EP-OBJECTIVE: {first_pose} ... {target_pose}\")\n",
    "            \n",
    "            # Format instructions\n",
    "            instructions = [(n[\"node_id\"], n[\"instruction_vector\"]) for n in path]\n",
    "            \n",
    "            episode_reward = []\n",
    "            episode_log_prob = []            \n",
    "            success_flag = False\n",
    "            \n",
    "            # The timesteps are attempts given to the model to find the target pose node\n",
    "            for t in range(max_timesteps):\n",
    "                used_timesteps.append(t+1)\n",
    "                prev_pose = current_pose\n",
    "    \n",
    "                # Pass forward model\n",
    "                graph.construct_torch_graph()\n",
    "                # Obtain local viewpoint\n",
    "                agent_state, ids = graph.subgraph(prev_pose)\n",
    "                agent_state.construct_torch_graph()\n",
    "                \n",
    "                # DEBUG\n",
    "                if False and TRAIN_DEBUG:\n",
    "                    print(\"-\" * 30)\n",
    "                    print(t)\n",
    "                    plt.figure()\n",
    "                    agent_state.draw()\n",
    "                \n",
    "                # TODO: Check all instructions\n",
    "                # Currently I am giving the instruction at the node relevant to it (EASY-ER MODE)\n",
    "                local_target = None\n",
    "                instruction = torch.zeros(1, node_repr_dim)\n",
    "                for i in range(len(instructions)):\n",
    "                    if instructions[i][0] == current_pose:\n",
    "                        # Get the ID of the next node in the path, its our next objective\n",
    "                        local_target_idx = min(i+1, len(instructions)-1)\n",
    "                        local_target = instructions[local_target_idx][0]\n",
    "                        instruction = instructions[i][1]\n",
    "                        \n",
    "                        # DEBUG\n",
    "                        if False and TRAIN_DEBUG:\n",
    "                            instruction_text = path[i]['instruction']\n",
    "                            print(f\"Instruction found! {current_pose} -> {local_target}({graph.node(local_target).name}) ~ {instruction_text}\")\n",
    "                        break\n",
    "\n",
    "                # Perform forward step\n",
    "                logits = model(agent_state._torch_graph.to(device), instruction).t()\n",
    "                \n",
    "                # we don't care about the landmark nodes, set their logits to something very small\n",
    "                mask = torch.tensor(agent_state.pose_mask()==0, dtype=torch.bool).to(device).view(1,-1) #removed before .to(device)\n",
    "                logits[mask] = -100\n",
    "                \n",
    "                # We are at our current pose, we don't go there again\n",
    "                if current_pose is not None:\n",
    "                    # Find current pose as local pose in ids\n",
    "                    local_pose_idx = (ids == current_pose).nonzero().item()\n",
    "                    logits[0,local_pose_idx] = -100\n",
    "                \n",
    "\n",
    "                if TRAIN_DEBUG and False:\n",
    "                    a = np.array(instruction)\n",
    "                    cos_vals = []\n",
    "                    for n in agent_state._nodes.values():\n",
    "                        cos_sim = np.dot(a, np.array(n.repr))/(np.linalg.norm(a)*np.linalg.norm(np.array(n.repr)))\n",
    "                        cos_vals.append(cos_sim[0])\n",
    "                        print(f'Instruction cosine distance to {n.name} = {cos_sim}')\n",
    "\n",
    "                    labels = [n.name for n in agent_state._nodes.values()]\n",
    "                    probs = F.softmax(logits.squeeze(), dim=0).tolist()\n",
    "                    x = np.arange(len(probs))  # the label locations\n",
    "                    width = 0.35  # the width of the bars\n",
    "                    fig, ax = plt.subplots()\n",
    "                    policy_dist = ax.bar(x - width/2, probs, width, label='Policy Action Distribution')\n",
    "                    cos_dist = ax.bar(x + width/2, cos_vals, width, label='Instruction-Landmark Cosine Similarity')\n",
    "                    ax.set_xticks(x)\n",
    "                    ax.set_xticklabels(labels)\n",
    "                    ax.legend()\n",
    "                    fig.tight_layout()\n",
    "                    plt.show()\n",
    "                \n",
    "                # sample a navigation goal from the output of the network\n",
    "                distribution = torch.distributions.Categorical(logits=logits)    \n",
    "                action = distribution.sample()\n",
    "                log_prob = distribution.log_prob(action)\n",
    "                \n",
    "                # Perform navigation action / selection\n",
    "                pose_index = action.item()\n",
    "                current_pose = ids[pose_index].item()\n",
    "                \n",
    "                # DEBUG PATH\n",
    "                if False and TRAIN_DEBUG:\n",
    "                    print(f\"ACTION: {prev_pose}({graph.node(prev_pose).name}) -> {current_pose}({graph.node(current_pose).name}) (local goal:{local_target}) (goal:{target_pose}({graph.node(target_pose).name}))\")\n",
    "\n",
    "                if first_pose is None:\n",
    "                    first_pose = current_pose\n",
    "                \n",
    "                # Check if the current_pose is the target_pose by comparing node ids\n",
    "                if current_pose == local_target or current_pose == target_pose:\n",
    "                    r = 1.0\n",
    "                    episode_reward.append(r)\n",
    "                    episode_log_prob.append(log_prob)\n",
    "                    reward.append(r)\n",
    "                    \n",
    "                    if np.sum(episode_reward) == path_length:\n",
    "                        num_successes += 1\n",
    "                        #print(f\"!!!Path Followed*** (not actually)!!! @{episode_num} CumSumReward@{np.sum(episode_reward)} SuccRate@ {num_successes}/{episode_num+1}\")\n",
    "                        # TODO: Maybe add a reward \n",
    "                        success_flag = True\n",
    "                        #break\n",
    "                else: \n",
    "                    r = -1.0\n",
    "                    episode_reward.append(r)\n",
    "                    episode_log_prob.append(log_prob)\n",
    "                    reward.append(r)\n",
    "                    #break # Consider taking this out, but random behaviour now results in termination\n",
    "            \n",
    "            # Policy update using reward gathered over episode\n",
    "            # Action's rewards gathered over the episode are discounted and multiplied by their log probabilities\n",
    "            if learner:\n",
    "                returns = torch.tensor(discount_rewards(episode_reward, gamma=0.99))    \n",
    "                #returns = (returns - returns.mean())# / (returns.std() + np.finfo(np.float32).eps.item())\n",
    "                policy_loss = []\n",
    "\n",
    "                for log_prob, R in zip(episode_log_prob, returns):\n",
    "                    policy_loss.append(-log_prob * R)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss = torch.cat(policy_loss).sum()\n",
    "                loss.requires_grad_()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "            # MAJOR TODO: Test collecting the log_probs of all the actions then multiply by \n",
    "            # rewards multipled by discounts then SUM and learn at the end of the episode\n",
    "            # TODO: Use discount factor (decay to zero by episode end)\n",
    "            # Entropy loss?? The policy can always take the same action. So we add a small loss term to promote entropy \n",
    "            \n",
    "            # GRAPH WEIGHTS\n",
    "            #for m in model.modules():\n",
    "            #    if isinstance(m, nn.Linear):\n",
    "            #        plt.figure()\n",
    "            #        plt.matshow(m.weight.data.detach().numpy())\n",
    "                    #print(m.weight.data.shape)\n",
    "            \n",
    "            #assert False\n",
    "            \n",
    "            ##########################\n",
    "            # Success should not be measured by reaching the target. Instead, if the path is followed.\n",
    "            # But in GENERAL success should be too important for this project right now.\n",
    "            # For example: It is hard to define \"success\" for cartpole\n",
    "            \"\"\"\n",
    "            CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials.\n",
    "            \"\"\"\n",
    "            ##########################\n",
    "            \n",
    "            if success_flag:\n",
    "                success.append(1)\n",
    "            else:\n",
    "                success.append(0)\n",
    "\n",
    "            if episode_num % 100 == 0 and episode_num != 0:\n",
    "                \n",
    "                # Record training data\n",
    "                dataframe = dataframe.append({'session':session, 'run':run, 'environment':environment_name,\n",
    "                                              'policy':experiment_name, 'mode': 'train', 'episode':episode_num,\n",
    "                                              'reward':np.mean(reward), 'success':np.mean(success),\n",
    "                                              'time':np.mean(used_timesteps)\n",
    "                                              },\n",
    "                                             ignore_index=True)            \n",
    "                \n",
    "                avg_r = np.mean(reward)\n",
    "                print(f\"Episode: {episode_num}\\tLast Reward: {sum(episode_reward)}\\tAverage Reward(-1 - 1): {avg_r}\")\n",
    "                \n",
    "                if avg_r > best_avg_r:\n",
    "                    print(f\"SAVING BEST AGENT: {datetime.now()}\")\n",
    "                    torch.save(model.state_dict(), 'agent-instructor-%s-%02d-%02d.pt' % (environment_name, session, run))       \n",
    "                    best_avg_r = avg_r\n",
    "\n",
    "                reward = []\n",
    "                used_timesteps = []\n",
    "                success = []\n",
    "\n",
    "        dataframe.to_pickle('dataframe-instructor-%s.pkl' % experiment_name)\n",
    "dataframe.to_pickle('dataframe-instructor-%s.pkl' % experiment_name)\n",
    "print(\"-\" * 50)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3102, 0.3402, 0.3497]], grad_fn=<TBackward>)\n",
      "tensor([[0.3043, 0.3691, 0.3266]], grad_fn=<TBackward>)\n",
      "tensor([[0.3290, 0.3365, 0.3345]], grad_fn=<TBackward>)\n",
      "tensor([[0.2999, 0.3882, 0.3119]], grad_fn=<TBackward>)\n",
      "tensor([[0.3098, 0.3269, 0.3633]], grad_fn=<TBackward>)\n",
      "-3.0\n",
      "tensor([[0.3463, 0.2778, 0.3759]], grad_fn=<TBackward>)\n",
      "tensor([[0.3280, 0.3517, 0.3203]], grad_fn=<TBackward>)\n",
      "tensor([[0.3322, 0.3274, 0.3404]], grad_fn=<TBackward>)\n",
      "tensor([[0.2703, 0.3506, 0.3791]], grad_fn=<TBackward>)\n",
      "tensor([[0.3555, 0.3256, 0.3189]], grad_fn=<TBackward>)\n",
      "-1.0\n",
      "tensor([[0.3881, 0.3055, 0.3065]], grad_fn=<TBackward>)\n",
      "tensor([[0.3476, 0.2703, 0.3821]], grad_fn=<TBackward>)\n",
      "tensor([[0.3335, 0.2810, 0.3855]], grad_fn=<TBackward>)\n",
      "tensor([[0.3510, 0.2611, 0.3879]], grad_fn=<TBackward>)\n",
      "tensor([[0.4232, 0.2664, 0.3104]], grad_fn=<TBackward>)\n",
      "-3.0\n",
      "tensor([[0.4407, 0.2572, 0.3021]], grad_fn=<TBackward>)\n",
      "tensor([[0.3410, 0.2455, 0.4135]], grad_fn=<TBackward>)\n",
      "tensor([[0.3697, 0.2829, 0.3474]], grad_fn=<TBackward>)\n",
      "tensor([[0.3883, 0.2993, 0.3123]], grad_fn=<TBackward>)\n",
      "tensor([[0.4023, 0.2899, 0.3078]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.4631, 0.2035, 0.3334]], grad_fn=<TBackward>)\n",
      "tensor([[0.4514, 0.2510, 0.2976]], grad_fn=<TBackward>)\n",
      "tensor([[0.3365, 0.2966, 0.3669]], grad_fn=<TBackward>)\n",
      "tensor([[0.4204, 0.2583, 0.3212]], grad_fn=<TBackward>)\n",
      "tensor([[0.2964, 0.3257, 0.3780]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.3447, 0.3299, 0.3254]], grad_fn=<TBackward>)\n",
      "tensor([[0.4529, 0.2841, 0.2630]], grad_fn=<TBackward>)\n",
      "tensor([[0.3480, 0.2952, 0.3568]], grad_fn=<TBackward>)\n",
      "tensor([[0.3555, 0.2848, 0.3598]], grad_fn=<TBackward>)\n",
      "tensor([[0.4298, 0.2528, 0.3174]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.4752, 0.2655, 0.2593]], grad_fn=<TBackward>)\n",
      "tensor([[0.3941, 0.3080, 0.2979]], grad_fn=<TBackward>)\n",
      "tensor([[0.4161, 0.2514, 0.3325]], grad_fn=<TBackward>)\n",
      "tensor([[0.4731, 0.2264, 0.3004]], grad_fn=<TBackward>)\n",
      "tensor([[0.4227, 0.2730, 0.3043]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.4417, 0.2035, 0.3548]], grad_fn=<TBackward>)\n",
      "tensor([[0.4197, 0.2659, 0.3144]], grad_fn=<TBackward>)\n",
      "tensor([[0.4670, 0.3154, 0.2176]], grad_fn=<TBackward>)\n",
      "tensor([[0.5158, 0.2085, 0.2757]], grad_fn=<TBackward>)\n",
      "tensor([[0.5372, 0.2468, 0.2160]], grad_fn=<TBackward>)\n",
      "-5.0\n",
      "tensor([[0.5346, 0.1943, 0.2711]], grad_fn=<TBackward>)\n",
      "tensor([[0.5345, 0.2581, 0.2074]], grad_fn=<TBackward>)\n",
      "tensor([[0.3890, 0.3238, 0.2871]], grad_fn=<TBackward>)\n",
      "tensor([[0.4468, 0.2808, 0.2724]], grad_fn=<TBackward>)\n",
      "tensor([[0.4687, 0.2658, 0.2655]], grad_fn=<TBackward>)\n",
      "-1.0\n",
      "tensor([[0.4662, 0.2724, 0.2614]], grad_fn=<TBackward>)\n",
      "tensor([[0.4235, 0.2920, 0.2845]], grad_fn=<TBackward>)\n",
      "tensor([[0.5562, 0.2339, 0.2100]], grad_fn=<TBackward>)\n",
      "tensor([[0.5229, 0.2270, 0.2502]], grad_fn=<TBackward>)\n",
      "tensor([[0.4814, 0.2170, 0.3016]], grad_fn=<TBackward>)\n",
      "-1.0\n",
      "tensor([[0.4820, 0.2034, 0.3146]], grad_fn=<TBackward>)\n",
      "tensor([[0.5563, 0.1756, 0.2681]], grad_fn=<TBackward>)\n",
      "tensor([[0.5093, 0.2775, 0.2132]], grad_fn=<TBackward>)\n",
      "tensor([[0.6172, 0.1763, 0.2065]], grad_fn=<TBackward>)\n",
      "tensor([[0.5035, 0.2105, 0.2860]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.4657, 0.3132, 0.2210]], grad_fn=<TBackward>)\n",
      "tensor([[0.5998, 0.1560, 0.2442]], grad_fn=<TBackward>)\n",
      "tensor([[0.6117, 0.1663, 0.2220]], grad_fn=<TBackward>)\n",
      "tensor([[0.5481, 0.2816, 0.1703]], grad_fn=<TBackward>)\n",
      "tensor([[0.5845, 0.1970, 0.2186]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.4702, 0.2831, 0.2466]], grad_fn=<TBackward>)\n",
      "tensor([[0.6582, 0.1914, 0.1504]], grad_fn=<TBackward>)\n",
      "tensor([[0.6274, 0.1837, 0.1889]], grad_fn=<TBackward>)\n",
      "tensor([[0.6819, 0.1543, 0.1639]], grad_fn=<TBackward>)\n",
      "tensor([[0.5380, 0.2151, 0.2469]], grad_fn=<TBackward>)\n",
      "-1.0\n",
      "tensor([[0.5843, 0.2067, 0.2090]], grad_fn=<TBackward>)\n",
      "tensor([[0.5546, 0.2434, 0.2020]], grad_fn=<TBackward>)\n",
      "tensor([[0.6166, 0.1724, 0.2110]], grad_fn=<TBackward>)\n",
      "tensor([[0.7478, 0.1254, 0.1268]], grad_fn=<TBackward>)\n",
      "tensor([[0.5080, 0.2412, 0.2508]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.6142, 0.1982, 0.1876]], grad_fn=<TBackward>)\n",
      "tensor([[0.6456, 0.1641, 0.1903]], grad_fn=<TBackward>)\n",
      "tensor([[0.7279, 0.1292, 0.1429]], grad_fn=<TBackward>)\n",
      "tensor([[0.5199, 0.2073, 0.2728]], grad_fn=<TBackward>)\n",
      "tensor([[0.6954, 0.1736, 0.1310]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.6245, 0.2264, 0.1491]], grad_fn=<TBackward>)\n",
      "tensor([[0.6145, 0.2193, 0.1662]], grad_fn=<TBackward>)\n",
      "tensor([[0.5743, 0.2539, 0.1718]], grad_fn=<TBackward>)\n",
      "tensor([[0.5496, 0.2031, 0.2474]], grad_fn=<TBackward>)\n",
      "tensor([[0.7264, 0.1066, 0.1670]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.5895, 0.1760, 0.2345]], grad_fn=<TBackward>)\n",
      "tensor([[0.7233, 0.1506, 0.1260]], grad_fn=<TBackward>)\n",
      "tensor([[0.7100, 0.1621, 0.1279]], grad_fn=<TBackward>)\n",
      "tensor([[0.6750, 0.1561, 0.1689]], grad_fn=<TBackward>)\n",
      "tensor([[0.7196, 0.1521, 0.1283]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.6668, 0.1562, 0.1770]], grad_fn=<TBackward>)\n",
      "tensor([[0.8339, 0.0866, 0.0795]], grad_fn=<TBackward>)\n",
      "tensor([[0.7832, 0.1024, 0.1143]], grad_fn=<TBackward>)\n",
      "tensor([[0.8018, 0.1175, 0.0807]], grad_fn=<TBackward>)\n",
      "tensor([[0.7508, 0.1413, 0.1079]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.7277, 0.1467, 0.1256]], grad_fn=<TBackward>)\n",
      "tensor([[0.7867, 0.0899, 0.1234]], grad_fn=<TBackward>)\n",
      "tensor([[0.6743, 0.1985, 0.1272]], grad_fn=<TBackward>)\n",
      "tensor([[0.7116, 0.1606, 0.1277]], grad_fn=<TBackward>)\n",
      "tensor([[0.7951, 0.1133, 0.0916]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.8431, 0.0885, 0.0684]], grad_fn=<TBackward>)\n",
      "tensor([[0.7489, 0.1507, 0.1004]], grad_fn=<TBackward>)\n",
      "tensor([[0.8682, 0.0747, 0.0570]], grad_fn=<TBackward>)\n",
      "tensor([[0.8401, 0.0670, 0.0930]], grad_fn=<TBackward>)\n",
      "tensor([[0.7841, 0.1039, 0.1120]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.8378, 0.0952, 0.0670]], grad_fn=<TBackward>)\n",
      "tensor([[0.8777, 0.0668, 0.0555]], grad_fn=<TBackward>)\n",
      "tensor([[0.8831, 0.0623, 0.0546]], grad_fn=<TBackward>)\n",
      "tensor([[0.9337, 0.0270, 0.0393]], grad_fn=<TBackward>)\n",
      "tensor([[0.8709, 0.0787, 0.0504]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.8095, 0.1132, 0.0773]], grad_fn=<TBackward>)\n",
      "tensor([[0.9129, 0.0521, 0.0349]], grad_fn=<TBackward>)\n",
      "tensor([[0.8009, 0.1039, 0.0953]], grad_fn=<TBackward>)\n",
      "tensor([[0.7944, 0.0975, 0.1082]], grad_fn=<TBackward>)\n",
      "tensor([[0.7651, 0.1082, 0.1267]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.8488, 0.0822, 0.0690]], grad_fn=<TBackward>)\n",
      "tensor([[0.8779, 0.0581, 0.0640]], grad_fn=<TBackward>)\n",
      "tensor([[0.8222, 0.0868, 0.0911]], grad_fn=<TBackward>)\n",
      "tensor([[0.8938, 0.0357, 0.0705]], grad_fn=<TBackward>)\n",
      "tensor([[0.7831, 0.1212, 0.0957]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.9298, 0.0339, 0.0363]], grad_fn=<TBackward>)\n",
      "tensor([[0.9203, 0.0350, 0.0448]], grad_fn=<TBackward>)\n",
      "tensor([[0.7389, 0.1450, 0.1161]], grad_fn=<TBackward>)\n",
      "tensor([[0.8745, 0.0816, 0.0439]], grad_fn=<TBackward>)\n",
      "tensor([[0.8657, 0.0437, 0.0906]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.8644, 0.0670, 0.0686]], grad_fn=<TBackward>)\n",
      "tensor([[0.9440, 0.0313, 0.0248]], grad_fn=<TBackward>)\n",
      "tensor([[0.8887, 0.0807, 0.0306]], grad_fn=<TBackward>)\n",
      "tensor([[0.9463, 0.0338, 0.0199]], grad_fn=<TBackward>)\n",
      "tensor([[0.9283, 0.0336, 0.0381]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.8515, 0.1118, 0.0367]], grad_fn=<TBackward>)\n",
      "tensor([[0.7026, 0.1524, 0.1450]], grad_fn=<TBackward>)\n",
      "tensor([[0.9783, 0.0098, 0.0118]], grad_fn=<TBackward>)\n",
      "tensor([[0.7439, 0.1847, 0.0714]], grad_fn=<TBackward>)\n",
      "tensor([[0.9555, 0.0302, 0.0143]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9359, 0.0360, 0.0281]], grad_fn=<TBackward>)\n",
      "tensor([[0.9488, 0.0384, 0.0128]], grad_fn=<TBackward>)\n",
      "tensor([[0.9783, 0.0098, 0.0119]], grad_fn=<TBackward>)\n",
      "tensor([[0.9106, 0.0582, 0.0312]], grad_fn=<TBackward>)\n",
      "tensor([[0.8508, 0.0504, 0.0988]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.8479, 0.0869, 0.0652]], grad_fn=<TBackward>)\n",
      "tensor([[0.8505, 0.0926, 0.0569]], grad_fn=<TBackward>)\n",
      "tensor([[0.8314, 0.1103, 0.0583]], grad_fn=<TBackward>)\n",
      "tensor([[0.8589, 0.0851, 0.0561]], grad_fn=<TBackward>)\n",
      "tensor([[0.6251, 0.1787, 0.1962]], grad_fn=<TBackward>)\n",
      "1.0\n",
      "tensor([[0.9292, 0.0250, 0.0458]], grad_fn=<TBackward>)\n",
      "tensor([[0.9589, 0.0289, 0.0122]], grad_fn=<TBackward>)\n",
      "tensor([[0.9167, 0.0285, 0.0548]], grad_fn=<TBackward>)\n",
      "tensor([[0.7595, 0.1514, 0.0890]], grad_fn=<TBackward>)\n",
      "tensor([[0.9570, 0.0200, 0.0230]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.9627, 0.0138, 0.0235]], grad_fn=<TBackward>)\n",
      "tensor([[0.9660, 0.0190, 0.0150]], grad_fn=<TBackward>)\n",
      "tensor([[0.9623, 0.0183, 0.0195]], grad_fn=<TBackward>)\n",
      "tensor([[0.8991, 0.0460, 0.0549]], grad_fn=<TBackward>)\n",
      "tensor([[0.9605, 0.0224, 0.0171]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.9687, 0.0157, 0.0157]], grad_fn=<TBackward>)\n",
      "tensor([[0.9583, 0.0105, 0.0311]], grad_fn=<TBackward>)\n",
      "tensor([[0.9384, 0.0157, 0.0459]], grad_fn=<TBackward>)\n",
      "tensor([[0.8768, 0.0659, 0.0573]], grad_fn=<TBackward>)\n",
      "tensor([[0.9550, 0.0258, 0.0192]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9665, 0.0101, 0.0234]], grad_fn=<TBackward>)\n",
      "tensor([[0.9856, 0.0065, 0.0079]], grad_fn=<TBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9650, 0.0214, 0.0136]], grad_fn=<TBackward>)\n",
      "tensor([[0.9898, 0.0053, 0.0049]], grad_fn=<TBackward>)\n",
      "tensor([[0.9579, 0.0151, 0.0269]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9562, 0.0194, 0.0244]], grad_fn=<TBackward>)\n",
      "tensor([[0.9892, 0.0032, 0.0076]], grad_fn=<TBackward>)\n",
      "tensor([[0.9512, 0.0160, 0.0327]], grad_fn=<TBackward>)\n",
      "tensor([[0.8905, 0.0502, 0.0593]], grad_fn=<TBackward>)\n",
      "tensor([[0.9652, 0.0199, 0.0149]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9687, 0.0093, 0.0220]], grad_fn=<TBackward>)\n",
      "tensor([[0.9532, 0.0234, 0.0234]], grad_fn=<TBackward>)\n",
      "tensor([[0.9484, 0.0345, 0.0171]], grad_fn=<TBackward>)\n",
      "tensor([[0.9819, 0.0089, 0.0092]], grad_fn=<TBackward>)\n",
      "tensor([[0.9155, 0.0387, 0.0458]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9587, 0.0194, 0.0220]], grad_fn=<TBackward>)\n",
      "tensor([[0.9564, 0.0238, 0.0198]], grad_fn=<TBackward>)\n",
      "tensor([[0.9721, 0.0219, 0.0061]], grad_fn=<TBackward>)\n",
      "tensor([[0.9922, 0.0025, 0.0052]], grad_fn=<TBackward>)\n",
      "tensor([[0.9764, 0.0117, 0.0119]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9711, 0.0067, 0.0222]], grad_fn=<TBackward>)\n",
      "tensor([[0.8598, 0.1033, 0.0369]], grad_fn=<TBackward>)\n",
      "tensor([[0.9704, 0.0215, 0.0081]], grad_fn=<TBackward>)\n",
      "tensor([[0.9801, 0.0158, 0.0041]], grad_fn=<TBackward>)\n",
      "tensor([[0.9566, 0.0261, 0.0173]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9298, 0.0400, 0.0302]], grad_fn=<TBackward>)\n",
      "tensor([[0.9869, 0.0076, 0.0055]], grad_fn=<TBackward>)\n",
      "tensor([[0.9246, 0.0264, 0.0491]], grad_fn=<TBackward>)\n",
      "tensor([[0.9861, 0.0098, 0.0041]], grad_fn=<TBackward>)\n",
      "tensor([[0.9868, 0.0072, 0.0060]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9821, 0.0073, 0.0106]], grad_fn=<TBackward>)\n",
      "tensor([[0.9217, 0.0308, 0.0475]], grad_fn=<TBackward>)\n",
      "tensor([[0.9494, 0.0255, 0.0251]], grad_fn=<TBackward>)\n",
      "tensor([[0.9845, 0.0110, 0.0044]], grad_fn=<TBackward>)\n",
      "tensor([[0.9485, 0.0123, 0.0391]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9883, 0.0027, 0.0089]], grad_fn=<TBackward>)\n",
      "tensor([[0.9926, 0.0044, 0.0030]], grad_fn=<TBackward>)\n",
      "tensor([[0.9200, 0.0457, 0.0343]], grad_fn=<TBackward>)\n",
      "tensor([[0.9724, 0.0202, 0.0075]], grad_fn=<TBackward>)\n",
      "tensor([[0.9815, 0.0137, 0.0048]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.9854, 0.0079, 0.0067]], grad_fn=<TBackward>)\n",
      "tensor([[0.9676, 0.0231, 0.0093]], grad_fn=<TBackward>)\n",
      "tensor([[0.9967, 0.0013, 0.0020]], grad_fn=<TBackward>)\n",
      "tensor([[0.9717, 0.0167, 0.0117]], grad_fn=<TBackward>)\n",
      "tensor([[0.9753, 0.0137, 0.0110]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9871, 0.0064, 0.0065]], grad_fn=<TBackward>)\n",
      "tensor([[0.9656, 0.0139, 0.0204]], grad_fn=<TBackward>)\n",
      "tensor([[0.9824, 0.0074, 0.0101]], grad_fn=<TBackward>)\n",
      "tensor([[0.9802, 0.0167, 0.0031]], grad_fn=<TBackward>)\n",
      "tensor([[0.9850, 0.0065, 0.0085]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9654e-01, 6.7758e-04, 2.7860e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9568, 0.0259, 0.0173]], grad_fn=<TBackward>)\n",
      "tensor([[0.9755, 0.0057, 0.0188]], grad_fn=<TBackward>)\n",
      "tensor([[0.9885, 0.0074, 0.0041]], grad_fn=<TBackward>)\n",
      "tensor([[0.9900, 0.0058, 0.0042]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.9377, 0.0127, 0.0495]], grad_fn=<TBackward>)\n",
      "tensor([[0.9577, 0.0247, 0.0176]], grad_fn=<TBackward>)\n",
      "tensor([[0.9970, 0.0014, 0.0016]], grad_fn=<TBackward>)\n",
      "tensor([[0.9592, 0.0239, 0.0169]], grad_fn=<TBackward>)\n",
      "tensor([[0.9924, 0.0054, 0.0022]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.9834, 0.0047, 0.0118]], grad_fn=<TBackward>)\n",
      "tensor([[0.9875, 0.0061, 0.0064]], grad_fn=<TBackward>)\n",
      "tensor([[0.9470, 0.0193, 0.0337]], grad_fn=<TBackward>)\n",
      "tensor([[0.9723, 0.0202, 0.0075]], grad_fn=<TBackward>)\n",
      "tensor([[0.9945, 0.0030, 0.0025]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9713e-01, 1.8982e-03, 9.7538e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9875, 0.0026, 0.0099]], grad_fn=<TBackward>)\n",
      "tensor([[0.9711, 0.0082, 0.0206]], grad_fn=<TBackward>)\n",
      "tensor([[0.9576, 0.0239, 0.0185]], grad_fn=<TBackward>)\n",
      "tensor([[0.9858, 0.0101, 0.0041]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9746e-01, 1.5684e-03, 9.6833e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9961, 0.0013, 0.0025]], grad_fn=<TBackward>)\n",
      "tensor([[0.9718, 0.0145, 0.0137]], grad_fn=<TBackward>)\n",
      "tensor([[9.9696e-01, 9.2745e-04, 2.1125e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9795, 0.0097, 0.0108]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9538, 0.0313, 0.0148]], grad_fn=<TBackward>)\n",
      "tensor([[0.9915, 0.0024, 0.0061]], grad_fn=<TBackward>)\n",
      "tensor([[0.9910, 0.0018, 0.0072]], grad_fn=<TBackward>)\n",
      "tensor([[0.9952, 0.0026, 0.0022]], grad_fn=<TBackward>)\n",
      "tensor([[9.9763e-01, 8.3263e-04, 1.5350e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9596, 0.0239, 0.0166]], grad_fn=<TBackward>)\n",
      "tensor([[9.9806e-01, 8.4999e-04, 1.0894e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9965, 0.0015, 0.0021]], grad_fn=<TBackward>)\n",
      "tensor([[0.9898, 0.0024, 0.0078]], grad_fn=<TBackward>)\n",
      "tensor([[0.9723, 0.0179, 0.0099]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9535, 0.0216, 0.0249]], grad_fn=<TBackward>)\n",
      "tensor([[0.9555, 0.0208, 0.0238]], grad_fn=<TBackward>)\n",
      "tensor([[0.9967, 0.0018, 0.0015]], grad_fn=<TBackward>)\n",
      "tensor([[0.9521, 0.0200, 0.0279]], grad_fn=<TBackward>)\n",
      "tensor([[0.9508, 0.0194, 0.0298]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9912, 0.0041, 0.0047]], grad_fn=<TBackward>)\n",
      "tensor([[0.9911, 0.0045, 0.0045]], grad_fn=<TBackward>)\n",
      "tensor([[0.9875, 0.0054, 0.0071]], grad_fn=<TBackward>)\n",
      "tensor([[0.9821, 0.0085, 0.0094]], grad_fn=<TBackward>)\n",
      "tensor([[9.9790e-01, 5.0239e-04, 1.5969e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9910, 0.0012, 0.0077]], grad_fn=<TBackward>)\n",
      "tensor([[0.9826, 0.0017, 0.0157]], grad_fn=<TBackward>)\n",
      "tensor([[0.9939, 0.0019, 0.0042]], grad_fn=<TBackward>)\n",
      "tensor([[0.9797, 0.0125, 0.0078]], grad_fn=<TBackward>)\n",
      "tensor([[0.9885, 0.0055, 0.0060]], grad_fn=<TBackward>)\n",
      "3.0\n",
      "tensor([[0.9954, 0.0017, 0.0029]], grad_fn=<TBackward>)\n",
      "tensor([[0.9737, 0.0067, 0.0196]], grad_fn=<TBackward>)\n",
      "tensor([[0.9893, 0.0036, 0.0071]], grad_fn=<TBackward>)\n",
      "tensor([[0.9806, 0.0071, 0.0122]], grad_fn=<TBackward>)\n",
      "tensor([[0.9900, 0.0025, 0.0074]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9877, 0.0101, 0.0022]], grad_fn=<TBackward>)\n",
      "tensor([[0.9953, 0.0027, 0.0021]], grad_fn=<TBackward>)\n",
      "tensor([[0.9949, 0.0025, 0.0026]], grad_fn=<TBackward>)\n",
      "tensor([[0.9801, 0.0024, 0.0175]], grad_fn=<TBackward>)\n",
      "tensor([[0.9850, 0.0071, 0.0079]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9560e-01, 5.8667e-04, 3.8143e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9954, 0.0011, 0.0034]], grad_fn=<TBackward>)\n",
      "tensor([[9.9813e-01, 2.5685e-04, 1.6133e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9955, 0.0017, 0.0028]], grad_fn=<TBackward>)\n",
      "tensor([[0.9615, 0.0340, 0.0045]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9876, 0.0054, 0.0070]], grad_fn=<TBackward>)\n",
      "tensor([[0.9958, 0.0020, 0.0022]], grad_fn=<TBackward>)\n",
      "tensor([[0.9758, 0.0113, 0.0128]], grad_fn=<TBackward>)\n",
      "tensor([[0.8993, 0.0616, 0.0391]], grad_fn=<TBackward>)\n",
      "tensor([[0.9972, 0.0011, 0.0017]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9598e-01, 4.6206e-04, 3.5533e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9824, 0.0104, 0.0072]], grad_fn=<TBackward>)\n",
      "tensor([[0.9858, 0.0119, 0.0023]], grad_fn=<TBackward>)\n",
      "tensor([[0.9776, 0.0161, 0.0063]], grad_fn=<TBackward>)\n",
      "tensor([[0.9606, 0.0168, 0.0226]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9912e-01, 4.6289e-04, 4.1450e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9909e-01, 5.6930e-04, 3.4206e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9795e-01, 6.5798e-04, 1.3948e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9937, 0.0018, 0.0045]], grad_fn=<TBackward>)\n",
      "tensor([[0.9822, 0.0052, 0.0127]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9848, 0.0075, 0.0077]], grad_fn=<TBackward>)\n",
      "tensor([[0.9969, 0.0013, 0.0018]], grad_fn=<TBackward>)\n",
      "tensor([[0.9960, 0.0014, 0.0026]], grad_fn=<TBackward>)\n",
      "tensor([[0.9906, 0.0041, 0.0053]], grad_fn=<TBackward>)\n",
      "tensor([[0.9038, 0.0603, 0.0359]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9949, 0.0036, 0.0015]], grad_fn=<TBackward>)\n",
      "tensor([[9.9303e-01, 9.8838e-04, 5.9790e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9679, 0.0077, 0.0244]], grad_fn=<TBackward>)\n",
      "tensor([[0.9942, 0.0025, 0.0033]], grad_fn=<TBackward>)\n",
      "tensor([[0.9802, 0.0053, 0.0145]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9819, 0.0132, 0.0049]], grad_fn=<TBackward>)\n",
      "tensor([[9.9869e-01, 4.5913e-04, 8.5328e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9483e-01, 6.4811e-04, 4.5209e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9753e-01, 4.2543e-04, 2.0462e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9776e-01, 3.3214e-04, 1.9127e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9914e-01, 4.2127e-04, 4.4242e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9888, 0.0038, 0.0074]], grad_fn=<TBackward>)\n",
      "tensor([[0.9759, 0.0047, 0.0194]], grad_fn=<TBackward>)\n",
      "tensor([[0.9964, 0.0021, 0.0015]], grad_fn=<TBackward>)\n",
      "tensor([[0.9961, 0.0014, 0.0025]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9925, 0.0037, 0.0038]], grad_fn=<TBackward>)\n",
      "tensor([[0.9963, 0.0014, 0.0023]], grad_fn=<TBackward>)\n",
      "tensor([[0.9822, 0.0048, 0.0130]], grad_fn=<TBackward>)\n",
      "tensor([[0.9457, 0.0282, 0.0260]], grad_fn=<TBackward>)\n",
      "tensor([[9.9893e-01, 6.3502e-04, 4.3274e-04]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9865, 0.0064, 0.0070]], grad_fn=<TBackward>)\n",
      "tensor([[0.9879, 0.0025, 0.0096]], grad_fn=<TBackward>)\n",
      "tensor([[0.9918, 0.0017, 0.0066]], grad_fn=<TBackward>)\n",
      "tensor([[9.8833e-01, 9.6823e-04, 1.0702e-02]], grad_fn=<TBackward>)\n",
      "tensor([[0.9690, 0.0072, 0.0238]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9882, 0.0060, 0.0058]], grad_fn=<TBackward>)\n",
      "tensor([[0.9732, 0.0113, 0.0155]], grad_fn=<TBackward>)\n",
      "tensor([[0.9908, 0.0026, 0.0066]], grad_fn=<TBackward>)\n",
      "tensor([[9.9581e-01, 8.5491e-04, 3.3329e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9938, 0.0037, 0.0025]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9909, 0.0024, 0.0067]], grad_fn=<TBackward>)\n",
      "tensor([[0.9874, 0.0077, 0.0049]], grad_fn=<TBackward>)\n",
      "tensor([[0.9866, 0.0069, 0.0065]], grad_fn=<TBackward>)\n",
      "tensor([[0.9909, 0.0031, 0.0060]], grad_fn=<TBackward>)\n",
      "tensor([[0.9814, 0.0079, 0.0107]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9556, 0.0096, 0.0348]], grad_fn=<TBackward>)\n",
      "tensor([[0.9973, 0.0013, 0.0014]], grad_fn=<TBackward>)\n",
      "tensor([[9.9540e-01, 4.3476e-04, 4.1690e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9897, 0.0052, 0.0051]], grad_fn=<TBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9401, 0.0055, 0.0544]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9899, 0.0055, 0.0046]], grad_fn=<TBackward>)\n",
      "tensor([[9.9930e-01, 1.9107e-04, 5.1105e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9785, 0.0091, 0.0125]], grad_fn=<TBackward>)\n",
      "tensor([[9.9960e-01, 1.9380e-04, 2.0255e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9959, 0.0030, 0.0011]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9935, 0.0028, 0.0037]], grad_fn=<TBackward>)\n",
      "tensor([[0.9948, 0.0018, 0.0034]], grad_fn=<TBackward>)\n",
      "tensor([[9.9750e-01, 5.7367e-04, 1.9233e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9949, 0.0037, 0.0014]], grad_fn=<TBackward>)\n",
      "tensor([[0.9941, 0.0030, 0.0029]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9814e-01, 4.3516e-04, 1.4204e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9958, 0.0024, 0.0018]], grad_fn=<TBackward>)\n",
      "tensor([[9.9678e-01, 2.6859e-03, 5.3284e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9954, 0.0027, 0.0019]], grad_fn=<TBackward>)\n",
      "tensor([[9.9852e-01, 7.8838e-04, 6.9172e-04]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9724e-01, 1.8097e-03, 9.4604e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9783, 0.0112, 0.0105]], grad_fn=<TBackward>)\n",
      "tensor([[0.9795, 0.0104, 0.0100]], grad_fn=<TBackward>)\n",
      "tensor([[0.9944, 0.0017, 0.0039]], grad_fn=<TBackward>)\n",
      "tensor([[0.9773, 0.0088, 0.0139]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9903, 0.0038, 0.0059]], grad_fn=<TBackward>)\n",
      "tensor([[9.9823e-01, 4.4209e-04, 1.3322e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9734, 0.0078, 0.0188]], grad_fn=<TBackward>)\n",
      "tensor([[0.9894, 0.0064, 0.0042]], grad_fn=<TBackward>)\n",
      "tensor([[0.9781, 0.0090, 0.0128]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9651e-01, 5.5378e-04, 2.9324e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9704, 0.0194, 0.0103]], grad_fn=<TBackward>)\n",
      "tensor([[0.9607, 0.0084, 0.0309]], grad_fn=<TBackward>)\n",
      "tensor([[0.9847, 0.0017, 0.0136]], grad_fn=<TBackward>)\n",
      "tensor([[9.9917e-01, 5.9740e-04, 2.3491e-04]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9718, 0.0121, 0.0161]], grad_fn=<TBackward>)\n",
      "tensor([[0.9962, 0.0019, 0.0019]], grad_fn=<TBackward>)\n",
      "tensor([[9.9769e-01, 8.0721e-04, 1.5013e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9958e-01, 1.8931e-04, 2.2992e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9927e-01, 3.2655e-04, 4.0479e-04]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9957, 0.0015, 0.0028]], grad_fn=<TBackward>)\n",
      "tensor([[9.9930e-01, 3.9373e-04, 3.0366e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9933, 0.0024, 0.0043]], grad_fn=<TBackward>)\n",
      "tensor([[9.9907e-01, 3.0074e-04, 6.2701e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9979, 0.0011, 0.0010]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9749, 0.0137, 0.0114]], grad_fn=<TBackward>)\n",
      "tensor([[0.9970, 0.0012, 0.0018]], grad_fn=<TBackward>)\n",
      "tensor([[9.9960e-01, 2.0893e-04, 1.9237e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9954, 0.0023, 0.0024]], grad_fn=<TBackward>)\n",
      "tensor([[0.9960, 0.0019, 0.0021]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9692, 0.0079, 0.0229]], grad_fn=<TBackward>)\n",
      "tensor([[9.9768e-01, 1.4627e-03, 8.5729e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9809, 0.0099, 0.0092]], grad_fn=<TBackward>)\n",
      "tensor([[9.9897e-01, 4.4429e-04, 5.8977e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9963, 0.0010, 0.0027]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9978, 0.0011, 0.0011]], grad_fn=<TBackward>)\n",
      "tensor([[9.9824e-01, 3.7220e-04, 1.3918e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9954, 0.0017, 0.0029]], grad_fn=<TBackward>)\n",
      "tensor([[0.9964, 0.0023, 0.0013]], grad_fn=<TBackward>)\n",
      "tensor([[0.9905, 0.0030, 0.0065]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9941, 0.0017, 0.0042]], grad_fn=<TBackward>)\n",
      "tensor([[9.9778e-01, 1.7092e-03, 5.0629e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9851, 0.0016, 0.0133]], grad_fn=<TBackward>)\n",
      "tensor([[9.9660e-01, 9.5051e-04, 2.4512e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9972, 0.0014, 0.0015]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9591e-01, 5.2516e-04, 3.5634e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9926, 0.0015, 0.0059]], grad_fn=<TBackward>)\n",
      "tensor([[0.9959, 0.0019, 0.0022]], grad_fn=<TBackward>)\n",
      "tensor([[9.9594e-01, 8.6164e-04, 3.1957e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9317e-01, 9.8221e-04, 5.8472e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9767, 0.0091, 0.0141]], grad_fn=<TBackward>)\n",
      "tensor([[9.9660e-01, 2.8743e-04, 3.1104e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9955, 0.0014, 0.0031]], grad_fn=<TBackward>)\n",
      "tensor([[9.9754e-01, 4.5343e-04, 2.0111e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9778e-01, 5.6048e-04, 1.6549e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.8953e-01, 8.5267e-04, 9.6137e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9959e-01, 2.0217e-04, 2.1234e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9927, 0.0033, 0.0040]], grad_fn=<TBackward>)\n",
      "tensor([[0.9864, 0.0062, 0.0074]], grad_fn=<TBackward>)\n",
      "tensor([[9.9686e-01, 5.0207e-04, 2.6420e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.8723e-01, 5.2869e-04, 1.2246e-02]], grad_fn=<TBackward>)\n",
      "tensor([[9.9922e-01, 2.0573e-04, 5.7488e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9942e-01, 3.3996e-04, 2.4337e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9978, 0.0010, 0.0011]], grad_fn=<TBackward>)\n",
      "tensor([[9.9868e-01, 2.6714e-04, 1.0550e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9973, 0.0014, 0.0013]], grad_fn=<TBackward>)\n",
      "tensor([[9.9914e-01, 2.7254e-04, 5.8574e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9978, 0.0012, 0.0010]], grad_fn=<TBackward>)\n",
      "tensor([[9.9811e-01, 6.8360e-04, 1.2061e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9821e-01, 6.4152e-04, 1.1505e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9855, 0.0089, 0.0055]], grad_fn=<TBackward>)\n",
      "tensor([[0.9909, 0.0025, 0.0067]], grad_fn=<TBackward>)\n",
      "tensor([[0.9971, 0.0016, 0.0013]], grad_fn=<TBackward>)\n",
      "tensor([[0.9726, 0.0113, 0.0161]], grad_fn=<TBackward>)\n",
      "tensor([[9.9668e-01, 6.9424e-04, 2.6307e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9894, 0.0050, 0.0056]], grad_fn=<TBackward>)\n",
      "tensor([[9.9856e-01, 1.0945e-03, 3.4485e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9869, 0.0092, 0.0039]], grad_fn=<TBackward>)\n",
      "tensor([[9.9931e-01, 3.2567e-04, 3.5971e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9699e-01, 3.3406e-04, 2.6772e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9896e-01, 4.0666e-04, 6.3727e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9958e-01, 7.4482e-05, 3.4273e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9839e-01, 7.2262e-04, 8.8476e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9856e-01, 5.3673e-04, 8.9961e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9787e-01, 4.4025e-04, 1.6868e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9824e-01, 1.1452e-03, 6.1680e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9885e-01, 4.2757e-04, 7.1966e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9877e-01, 3.7874e-04, 8.5150e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9905e-01, 1.6288e-04, 7.8602e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9972, 0.0017, 0.0011]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9980e-01, 1.2379e-04, 7.7221e-05]], grad_fn=<TBackward>)\n",
      "tensor([[0.9941, 0.0016, 0.0042]], grad_fn=<TBackward>)\n",
      "tensor([[0.9947, 0.0010, 0.0043]], grad_fn=<TBackward>)\n",
      "tensor([[9.9351e-01, 7.1492e-04, 5.7781e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9840e-01, 6.8050e-04, 9.1990e-04]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9975, 0.0014, 0.0011]], grad_fn=<TBackward>)\n",
      "tensor([[9.9728e-01, 6.2769e-04, 2.0915e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9872e-01, 2.0651e-04, 1.0718e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9880e-01, 4.2238e-04, 7.7500e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9840, 0.0014, 0.0146]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9933, 0.0015, 0.0052]], grad_fn=<TBackward>)\n",
      "tensor([[9.9932e-01, 1.5439e-04, 5.2167e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9960, 0.0020, 0.0019]], grad_fn=<TBackward>)\n",
      "tensor([[9.9749e-01, 4.3961e-04, 2.0749e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9939, 0.0016, 0.0045]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9941e-01, 9.0847e-05, 4.9521e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9970, 0.0019, 0.0011]], grad_fn=<TBackward>)\n",
      "tensor([[0.9880, 0.0028, 0.0091]], grad_fn=<TBackward>)\n",
      "tensor([[0.9958, 0.0019, 0.0023]], grad_fn=<TBackward>)\n",
      "tensor([[9.9698e-01, 6.7261e-04, 2.3503e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9797e-01, 9.7059e-04, 1.0550e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9979e-01, 1.5317e-04, 5.2498e-05]], grad_fn=<TBackward>)\n",
      "tensor([[9.9781e-01, 6.3247e-04, 1.5562e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9804e-01, 4.4209e-04, 1.5217e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9473e-01, 5.5045e-04, 4.7206e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9882e-01, 3.5377e-04, 8.2734e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9761e-01, 8.9567e-04, 1.4908e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9868e-01, 6.5188e-04, 6.6780e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9821e-01, 3.1155e-04, 1.4803e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9819e-01, 5.2604e-04, 1.2834e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9910, 0.0047, 0.0042]], grad_fn=<TBackward>)\n",
      "tensor([[0.9924, 0.0063, 0.0013]], grad_fn=<TBackward>)\n",
      "tensor([[9.9975e-01, 1.2863e-04, 1.2612e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9832e-01, 8.9307e-04, 7.9170e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9951e-01, 1.8546e-04, 3.0422e-04]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9978e-01, 4.1007e-05, 1.7967e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9924e-01, 1.5728e-04, 6.0642e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9910e-01, 1.5528e-04, 7.4345e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9705e-01, 5.6091e-04, 2.3867e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9805e-01, 2.7397e-04, 1.6782e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9845e-01, 2.5650e-04, 1.2951e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9796e-01, 1.0350e-04, 1.9393e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9926e-01, 3.6357e-04, 3.7719e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9570e-01, 7.5166e-04, 3.5491e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9895, 0.0029, 0.0077]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9963, 0.0013, 0.0023]], grad_fn=<TBackward>)\n",
      "tensor([[9.9907e-01, 1.1808e-04, 8.1327e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9864e-01, 1.8504e-04, 1.1755e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9758e-01, 1.6629e-03, 7.5840e-04]], grad_fn=<TBackward>)\n",
      "tensor([[9.9939e-01, 1.3354e-04, 4.7585e-04]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[9.9605e-01, 9.7050e-04, 2.9814e-03]], grad_fn=<TBackward>)\n",
      "tensor([[0.9890, 0.0082, 0.0028]], grad_fn=<TBackward>)\n",
      "tensor([[0.9939, 0.0029, 0.0032]], grad_fn=<TBackward>)\n",
      "tensor([[9.9831e-01, 5.8773e-04, 1.1009e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9837e-01, 4.8804e-04, 1.1438e-03]], grad_fn=<TBackward>)\n",
      "5.0\n",
      "tensor([[0.9842, 0.0122, 0.0036]], grad_fn=<TBackward>)\n",
      "tensor([[9.9381e-01, 9.7994e-04, 5.2146e-03]], grad_fn=<TBackward>)\n",
      "tensor([[9.9904e-01, 1.3899e-04, 8.1983e-04]], grad_fn=<TBackward>)\n",
      "tensor([[0.9890, 0.0038, 0.0072]], grad_fn=<TBackward>)\n",
      "tensor([[0.9909, 0.0016, 0.0076]], grad_fn=<TBackward>)\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK\n",
    "# Can a single FC layer learn cosine distance?\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return np.array(returns)\n",
    "        \n",
    "        \n",
    "\n",
    "class SANITY(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SANITY, self).__init__()\n",
    "        self.fc1 = nn.Linear(600,128)\n",
    "        self.drop1 = nn.Dropout(p=0.6)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "            \n",
    "    def forward(self, words, target):\n",
    "        concatenated = torch.cat((words, target.expand(3,-1)), dim=1) \n",
    "        x = self.fc1(concatenated)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=0)\n",
    "    \n",
    "model = SANITY()\n",
    "model.train()\n",
    "# set up the optimiser and the rest of the training pipeline\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "words=['apple','bird','train']\n",
    "word_vectors = torch.zeros(len(words), 300)\n",
    "for w in range(len(words)):\n",
    "    word_vectors[w] = fast_text.get_vecs_by_tokens(words[w]) \n",
    "    #print(np.amax(word_vectors[w].numpy()))\n",
    "    #print(np.amin(word_vectors[w].numpy()))\n",
    "\n",
    "    \n",
    "# Display debug of word vectors\n",
    "# are they suitable for NN without preprocess? (normalize to 0-1 or -1-1)\n",
    "#plt.figure(figsize=(18,3))\n",
    "#plt.matshow(word_vectors.numpy(), fignum=1, aspect='auto')\n",
    "\n",
    "succ = 0\n",
    "eps = 100\n",
    "timesteps = 5\n",
    "success = []\n",
    "cum_rewards = []\n",
    "running_reward = 10\n",
    "\n",
    "for i in range(1, eps):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    ep_reward = 0.0\n",
    "    for t in range(timesteps):\n",
    "        probs = model(word_vectors, word_vectors[0]).t()\n",
    "        print(probs)\n",
    "        \n",
    "        if False:\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    plt.figure()\n",
    "                    plt.matshow(m.weight.data.detach().numpy())\n",
    "                   #print(m.weight.data.shape)\n",
    "        #assert False\n",
    "        \n",
    "        distribution = torch.distributions.Categorical(probs=probs)    \n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action)\n",
    "        \n",
    "        if action.item() == 0:\n",
    "            r = 1.0\n",
    "            rewards.append(r)\n",
    "            log_probs.append(log_prob)\n",
    "            ep_reward += r\n",
    "        else:\n",
    "            r = -1.0\n",
    "            rewards.append(r)\n",
    "            log_probs.append(log_prob)\n",
    "            ep_reward += r\n",
    "            #break # Done\n",
    "\n",
    "    cum_rewards.append(ep_reward)\n",
    "    print(ep_reward)\n",
    "    \n",
    "    #print(rewards)\n",
    "    returns = torch.tensor(discount_rewards(rewards, gamma=0.99))    \n",
    "    #print(returns)\n",
    "    #returns = (returns - returns.mean())# / (returns.std() + np.finfo(np.float32).eps.item())\n",
    "    #print(returns)\n",
    "    policy_loss = []\n",
    "    \n",
    "    for log_prob, R in zip(log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss = torch.cat(policy_loss).sum()\n",
    "    #loss = -(torch.tensor(log_probs, requires_grad=True) * torch.tensor(discount_reward, requires_grad=True))\n",
    "    #loss = loss.sum()\n",
    "    #print(policy_loss)\n",
    "    #print(loss)\n",
    "    #break\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'Episode: {i}\\tLast Reward @ {ep_reward}\\tAverage Reward @ {running_reward}')\n",
    "        success = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.022581130266189575 tensor([[ 0.9949],\n",
      "        [-0.1446],\n",
      "        [ 0.0405]], grad_fn=<AddmmBackward>)\n",
      "199 0.010279283858835697 tensor([[ 1.0099],\n",
      "        [-0.0991],\n",
      "        [ 0.0191]], grad_fn=<AddmmBackward>)\n",
      "299 0.002404104918241501 tensor([[ 0.9607],\n",
      "        [ 0.0187],\n",
      "        [-0.0225]], grad_fn=<AddmmBackward>)\n",
      "399 0.01348860003054142 tensor([[ 1.0646],\n",
      "        [-0.0953],\n",
      "        [-0.0153]], grad_fn=<AddmmBackward>)\n",
      "499 0.013901088386774063 tensor([[ 0.9003],\n",
      "        [-0.0058],\n",
      "        [ 0.0627]], grad_fn=<AddmmBackward>)\n",
      "599 0.0016003736527636647 tensor([[ 9.9541e-01],\n",
      "        [ 8.7089e-04],\n",
      "        [-3.9731e-02]], grad_fn=<AddmmBackward>)\n",
      "699 0.005805738735944033 tensor([[0.9376],\n",
      "        [0.0433],\n",
      "        [0.0057]], grad_fn=<AddmmBackward>)\n",
      "799 0.015134768560528755 tensor([[ 0.8847],\n",
      "        [-0.0384],\n",
      "        [ 0.0194]], grad_fn=<AddmmBackward>)\n",
      "899 0.03333212807774544 tensor([[ 0.8180],\n",
      "        [-0.0092],\n",
      "        [-0.0118]], grad_fn=<AddmmBackward>)\n",
      "999 0.0017028303118422627 tensor([[ 1.0066],\n",
      "        [-0.0042],\n",
      "        [-0.0405]], grad_fn=<AddmmBackward>)\n",
      "1099 0.0016902689822018147 tensor([[ 0.9632],\n",
      "        [-0.0057],\n",
      "        [ 0.0174]], grad_fn=<AddmmBackward>)\n",
      "1199 0.009772567078471184 tensor([[ 0.9025],\n",
      "        [-0.0044],\n",
      "        [-0.0155]], grad_fn=<AddmmBackward>)\n",
      "1299 0.002287526149302721 tensor([[0.9618],\n",
      "        [0.0198],\n",
      "        [0.0209]], grad_fn=<AddmmBackward>)\n",
      "1399 0.0017741075716912746 tensor([[ 0.9665],\n",
      "        [-0.0071],\n",
      "        [ 0.0246]], grad_fn=<AddmmBackward>)\n",
      "1499 0.00014172076771501452 tensor([[ 1.0046],\n",
      "        [-0.0083],\n",
      "        [-0.0072]], grad_fn=<AddmmBackward>)\n",
      "1599 0.0031856338027864695 tensor([[0.9444],\n",
      "        [0.0060],\n",
      "        [0.0078]], grad_fn=<AddmmBackward>)\n",
      "1699 0.0003710008168127388 tensor([[ 0.9883],\n",
      "        [-0.0033],\n",
      "        [-0.0149]], grad_fn=<AddmmBackward>)\n",
      "1799 0.007046967279165983 tensor([[ 0.9170],\n",
      "        [ 0.0069],\n",
      "        [-0.0103]], grad_fn=<AddmmBackward>)\n",
      "1899 0.016661519184708595 tensor([[0.8710],\n",
      "        [0.0011],\n",
      "        [0.0029]], grad_fn=<AddmmBackward>)\n",
      "1999 0.017288554459810257 tensor([[1.1313],\n",
      "        [0.0063],\n",
      "        [0.0025]], grad_fn=<AddmmBackward>)\n",
      "2099 0.012988551519811153 tensor([[1.1135],\n",
      "        [0.0053],\n",
      "        [0.0089]], grad_fn=<AddmmBackward>)\n",
      "2199 0.0006212491425685585 tensor([[1.0233],\n",
      "        [0.0085],\n",
      "        [0.0022]], grad_fn=<AddmmBackward>)\n",
      "2299 0.00013947297702543437 tensor([[0.9976],\n",
      "        [0.0090],\n",
      "        [0.0073]], grad_fn=<AddmmBackward>)\n",
      "2399 0.00012259965296834707 tensor([[ 9.9439e-01],\n",
      "        [-9.5345e-03],\n",
      "        [-4.9008e-04]], grad_fn=<AddmmBackward>)\n",
      "2499 0.007744153495877981 tensor([[1.0858],\n",
      "        [0.0149],\n",
      "        [0.0127]], grad_fn=<AddmmBackward>)\n",
      "2599 0.0011375030735507607 tensor([[0.9683],\n",
      "        [0.0096],\n",
      "        [0.0062]], grad_fn=<AddmmBackward>)\n",
      "2699 5.1273218559799716e-05 tensor([[ 9.9296e-01],\n",
      "        [ 1.3146e-03],\n",
      "        [-1.8518e-04]], grad_fn=<AddmmBackward>)\n",
      "2799 0.00013339317229110748 tensor([[ 0.9952],\n",
      "        [-0.0046],\n",
      "        [ 0.0094]], grad_fn=<AddmmBackward>)\n",
      "2899 0.004531761631369591 tensor([[ 0.9335],\n",
      "        [ 0.0104],\n",
      "        [-0.0011]], grad_fn=<AddmmBackward>)\n",
      "2999 0.0005989480414427817 tensor([[ 9.7579e-01],\n",
      "        [ 1.3537e-04],\n",
      "        [-3.5835e-03]], grad_fn=<AddmmBackward>)\n",
      "3099 0.0070090037770569324 tensor([[ 1.0822],\n",
      "        [ 0.0087],\n",
      "        [-0.0131]], grad_fn=<AddmmBackward>)\n",
      "3199 0.003496300894767046 tensor([[1.0591],\n",
      "        [0.0017],\n",
      "        [0.0022]], grad_fn=<AddmmBackward>)\n",
      "3299 0.010604863986372948 tensor([[ 0.8974],\n",
      "        [-0.0050],\n",
      "        [ 0.0078]], grad_fn=<AddmmBackward>)\n",
      "3399 0.033260732889175415 tensor([[1.1822],\n",
      "        [0.0058],\n",
      "        [0.0042]], grad_fn=<AddmmBackward>)\n",
      "3499 0.011704879812896252 tensor([[ 0.8923],\n",
      "        [ 0.0076],\n",
      "        [-0.0069]], grad_fn=<AddmmBackward>)\n",
      "3599 0.031328242272138596 tensor([[ 0.8233],\n",
      "        [-0.0085],\n",
      "        [ 0.0063]], grad_fn=<AddmmBackward>)\n",
      "3699 1.3144882359483745e-05 tensor([[ 1.0016],\n",
      "        [-0.0030],\n",
      "        [ 0.0012]], grad_fn=<AddmmBackward>)\n",
      "3799 0.015127122402191162 tensor([[ 1.1221],\n",
      "        [ 0.0148],\n",
      "        [-0.0028]], grad_fn=<AddmmBackward>)\n",
      "3899 0.0013828086666762829 tensor([[ 0.9665],\n",
      "        [-0.0152],\n",
      "        [ 0.0056]], grad_fn=<AddmmBackward>)\n",
      "3999 0.0035962166730314493 tensor([[ 0.9406],\n",
      "        [ 0.0028],\n",
      "        [-0.0080]], grad_fn=<AddmmBackward>)\n",
      "4099 0.006116153672337532 tensor([[ 0.9243],\n",
      "        [ 0.0169],\n",
      "        [-0.0102]], grad_fn=<AddmmBackward>)\n",
      "4199 0.0003197191981598735 tensor([[1.0054],\n",
      "        [0.0075],\n",
      "        [0.0153]], grad_fn=<AddmmBackward>)\n",
      "4299 0.000773829233366996 tensor([[ 1.0251e+00],\n",
      "        [-1.2074e-02],\n",
      "        [ 6.0787e-04]], grad_fn=<AddmmBackward>)\n",
      "4399 0.0050981719978153706 tensor([[ 1.0708],\n",
      "        [-0.0054],\n",
      "        [ 0.0071]], grad_fn=<AddmmBackward>)\n",
      "4499 0.0009138177847489715 tensor([[ 0.9712],\n",
      "        [-0.0084],\n",
      "        [-0.0033]], grad_fn=<AddmmBackward>)\n",
      "4599 0.016285615041851997 tensor([[0.8726],\n",
      "        [0.0040],\n",
      "        [0.0056]], grad_fn=<AddmmBackward>)\n",
      "4699 0.0003667512210085988 tensor([[ 1.0174],\n",
      "        [-0.0076],\n",
      "        [ 0.0027]], grad_fn=<AddmmBackward>)\n",
      "4799 0.010438580065965652 tensor([[ 0.8985],\n",
      "        [-0.0028],\n",
      "        [-0.0117]], grad_fn=<AddmmBackward>)\n",
      "4899 0.00027423747815191746 tensor([[ 0.9928],\n",
      "        [ 0.0118],\n",
      "        [-0.0091]], grad_fn=<AddmmBackward>)\n",
      "4999 0.00031535723246634007 tensor([[ 9.9401e-01],\n",
      "        [ 4.5685e-04],\n",
      "        [-1.6710e-02]], grad_fn=<AddmmBackward>)\n",
      "5099 0.001029724720865488 tensor([[ 1.0293],\n",
      "        [-0.0048],\n",
      "        [-0.0121]], grad_fn=<AddmmBackward>)\n",
      "5199 0.0005784766981378198 tensor([[ 0.9926],\n",
      "        [ 0.0215],\n",
      "        [-0.0078]], grad_fn=<AddmmBackward>)\n",
      "5299 2.661190228536725e-05 tensor([[ 1.0032],\n",
      "        [-0.0039],\n",
      "        [-0.0013]], grad_fn=<AddmmBackward>)\n",
      "5399 0.0019307318143546581 tensor([[ 0.9577],\n",
      "        [ 0.0059],\n",
      "        [-0.0104]], grad_fn=<AddmmBackward>)\n",
      "5499 0.0012754978379234672 tensor([[ 0.9645],\n",
      "        [-0.0010],\n",
      "        [-0.0039]], grad_fn=<AddmmBackward>)\n",
      "5599 0.005412026774138212 tensor([[ 1.0734e+00],\n",
      "        [-4.5352e-03],\n",
      "        [ 3.3985e-04]], grad_fn=<AddmmBackward>)\n",
      "5699 0.007329227402806282 tensor([[1.0846],\n",
      "        [0.0123],\n",
      "        [0.0048]], grad_fn=<AddmmBackward>)\n",
      "5799 0.012597665190696716 tensor([[ 1.1117],\n",
      "        [-0.0030],\n",
      "        [-0.0106]], grad_fn=<AddmmBackward>)\n",
      "5899 0.00022594428446609527 tensor([[ 1.0132],\n",
      "        [-0.0054],\n",
      "        [-0.0047]], grad_fn=<AddmmBackward>)\n",
      "5999 0.006838114466518164 tensor([[0.9174],\n",
      "        [0.0025],\n",
      "        [0.0018]], grad_fn=<AddmmBackward>)\n",
      "6099 0.0012577627785503864 tensor([[1.0344],\n",
      "        [0.0014],\n",
      "        [0.0085]], grad_fn=<AddmmBackward>)\n",
      "6199 0.0022889473475515842 tensor([[ 1.0440],\n",
      "        [-0.0171],\n",
      "        [ 0.0079]], grad_fn=<AddmmBackward>)\n",
      "6299 0.0015144411008805037 tensor([[ 9.6163e-01],\n",
      "        [-3.6391e-04],\n",
      "        [-6.4751e-03]], grad_fn=<AddmmBackward>)\n",
      "6399 0.00489297928288579 tensor([[ 1.0685],\n",
      "        [-0.0093],\n",
      "        [-0.0106]], grad_fn=<AddmmBackward>)\n",
      "6499 0.017510998994112015 tensor([[ 8.6780e-01],\n",
      "        [ 5.8334e-03],\n",
      "        [-3.8237e-04]], grad_fn=<AddmmBackward>)\n",
      "6599 0.0010142165701836348 tensor([[ 0.9728],\n",
      "        [ 0.0161],\n",
      "        [-0.0039]], grad_fn=<AddmmBackward>)\n",
      "6699 0.0006802956922911108 tensor([[1.0163],\n",
      "        [0.0203],\n",
      "        [0.0011]], grad_fn=<AddmmBackward>)\n",
      "6799 0.00043559426558203995 tensor([[ 1.0156e+00],\n",
      "        [ 1.3830e-02],\n",
      "        [-4.4497e-04]], grad_fn=<AddmmBackward>)\n",
      "6899 3.891091182595119e-05 tensor([[ 1.0062e+00],\n",
      "        [ 1.2197e-05],\n",
      "        [-7.3032e-04]], grad_fn=<AddmmBackward>)\n",
      "6999 0.002433145185932517 tensor([[ 0.9555],\n",
      "        [ 0.0209],\n",
      "        [-0.0043]], grad_fn=<AddmmBackward>)\n",
      "7099 0.009657745249569416 tensor([[ 1.0971],\n",
      "        [-0.0137],\n",
      "        [ 0.0059]], grad_fn=<AddmmBackward>)\n",
      "7199 0.010205419734120369 tensor([[ 0.8991],\n",
      "        [-0.0048],\n",
      "        [-0.0012]], grad_fn=<AddmmBackward>)\n",
      "7299 0.029027467593550682 tensor([[ 1.1699],\n",
      "        [-0.0027],\n",
      "        [ 0.0125]], grad_fn=<AddmmBackward>)\n",
      "7399 0.0016621696995571256 tensor([[ 1.0404],\n",
      "        [-0.0029],\n",
      "        [-0.0047]], grad_fn=<AddmmBackward>)\n",
      "7499 0.006766766309738159 tensor([[ 0.9192],\n",
      "        [-0.0033],\n",
      "        [-0.0152]], grad_fn=<AddmmBackward>)\n",
      "7599 0.0003656111366581172 tensor([[ 1.0168e+00],\n",
      "        [-9.0895e-03],\n",
      "        [ 2.0359e-04]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7699 0.001467060879804194 tensor([[0.9621],\n",
      "        [0.0051],\n",
      "        [0.0015]], grad_fn=<AddmmBackward>)\n",
      "7799 0.0022930533159524202 tensor([[ 1.0468],\n",
      "        [-0.0088],\n",
      "        [-0.0048]], grad_fn=<AddmmBackward>)\n",
      "7899 0.021527308970689774 tensor([[1.1463e+00],\n",
      "        [1.8281e-04],\n",
      "        [1.1392e-02]], grad_fn=<AddmmBackward>)\n",
      "7999 0.02581442892551422 tensor([[0.8405],\n",
      "        [0.0082],\n",
      "        [0.0178]], grad_fn=<AddmmBackward>)\n",
      "8099 0.009095953777432442 tensor([[0.9058],\n",
      "        [0.0064],\n",
      "        [0.0133]], grad_fn=<AddmmBackward>)\n",
      "8199 0.004749976564198732 tensor([[ 1.0656],\n",
      "        [-0.0105],\n",
      "        [ 0.0182]], grad_fn=<AddmmBackward>)\n",
      "8299 0.0010116077028214931 tensor([[ 0.9763],\n",
      "        [-0.0212],\n",
      "        [-0.0011]], grad_fn=<AddmmBackward>)\n",
      "8399 0.0015298841753974557 tensor([[9.6169e-01],\n",
      "        [7.6535e-04],\n",
      "        [7.8568e-03]], grad_fn=<AddmmBackward>)\n",
      "8499 0.005042731761932373 tensor([[ 0.9329],\n",
      "        [-0.0199],\n",
      "        [ 0.0119]], grad_fn=<AddmmBackward>)\n",
      "8599 0.0014256179565563798 tensor([[ 0.9630],\n",
      "        [-0.0073],\n",
      "        [ 0.0020]], grad_fn=<AddmmBackward>)\n",
      "8699 0.0031903875060379505 tensor([[ 1.0488],\n",
      "        [ 0.0030],\n",
      "        [-0.0283]], grad_fn=<AddmmBackward>)\n",
      "8799 0.007636798080056906 tensor([[ 0.9129],\n",
      "        [-0.0060],\n",
      "        [-0.0029]], grad_fn=<AddmmBackward>)\n",
      "8899 0.0032071699388325214 tensor([[9.4348e-01],\n",
      "        [3.5287e-03],\n",
      "        [1.7464e-05]], grad_fn=<AddmmBackward>)\n",
      "8999 0.0023979824036359787 tensor([[1.0485],\n",
      "        [0.0020],\n",
      "        [0.0064]], grad_fn=<AddmmBackward>)\n",
      "9099 0.0013011140981689095 tensor([[ 1.0345],\n",
      "        [-0.0046],\n",
      "        [-0.0095]], grad_fn=<AddmmBackward>)\n",
      "9199 0.0027709268033504486 tensor([[ 1.0525e+00],\n",
      "        [-4.7099e-04],\n",
      "        [ 4.0304e-03]], grad_fn=<AddmmBackward>)\n",
      "9299 0.004084266722202301 tensor([[ 1.0624],\n",
      "        [-0.0112],\n",
      "        [ 0.0083]], grad_fn=<AddmmBackward>)\n",
      "9399 0.0015159413451328874 tensor([[ 1.0328],\n",
      "        [-0.0165],\n",
      "        [-0.0130]], grad_fn=<AddmmBackward>)\n",
      "9499 0.0002679444442037493 tensor([[ 1.0163e+00],\n",
      "        [-9.5256e-04],\n",
      "        [-1.4604e-03]], grad_fn=<AddmmBackward>)\n",
      "9599 0.01096252165734768 tensor([[0.8959],\n",
      "        [0.0017],\n",
      "        [0.0112]], grad_fn=<AddmmBackward>)\n",
      "9699 0.003999513108283281 tensor([[1.0624],\n",
      "        [0.0099],\n",
      "        [0.0029]], grad_fn=<AddmmBackward>)\n",
      "9799 0.0011931380722671747 tensor([[1.0259],\n",
      "        [0.0047],\n",
      "        [0.0223]], grad_fn=<AddmmBackward>)\n",
      "9899 0.0004375627904664725 tensor([[0.9875],\n",
      "        [0.0102],\n",
      "        [0.0133]], grad_fn=<AddmmBackward>)\n",
      "9999 0.006479701958596706 tensor([[0.9229],\n",
      "        [0.0181],\n",
      "        [0.0145]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK\n",
    "# Can a single FC layer learn cosine distance?\n",
    "\n",
    "class SANITY(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SANITY, self).__init__()\n",
    "        self.fc1 = nn.Linear(600,128)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "            \n",
    "    def forward(self, words, target):\n",
    "        concatenated = torch.cat((words, target.expand(3,-1)), dim=1) \n",
    "        x = self.fc1(concatenated)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = SANITY()\n",
    "model.train()\n",
    "# set up the optimiser and the rest of the training pipeline\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "words=['apple','bird','train']\n",
    "word_vectors = torch.zeros(len(words), 300)\n",
    "for w in range(len(words)):\n",
    "    word_vectors[w] = fast_text.get_vecs_by_tokens(words[w]) \n",
    "\n",
    "succ = 0\n",
    "eps = 10000\n",
    "timesteps = 3\n",
    "success = []\n",
    "    \n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(eps):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(word_vectors, word_vectors[0])\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, torch.tensor([[1],[0],[0]], dtype=torch.float))\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item(), y_pred)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/Users/riordan/Desktop/datasets/glove.6B/glove.6B.50d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.50d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bathroom', 0.8193663954734802),\n",
       " ('shop', 0.8010247349739075),\n",
       " ('room', 0.7953479290008545),\n",
       " ('laundry', 0.7935865521430969),\n",
       " ('dining', 0.7767717838287354),\n",
       " ('kitchens', 0.7646009922027588),\n",
       " ('cleaning', 0.7608016729354858),\n",
       " ('cooking', 0.7597110867500305),\n",
       " ('furniture', 0.7554135918617249),\n",
       " ('upstairs', 0.7415493130683899)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"kitchen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text.get_vecs_by_tokens(\"hair\").shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
