{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the ALFRED dataset\n",
    "import tqdm\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "COMPUTER = 'MAC'\n",
    "PC_ALFRED_DATA = 'D:\\\\Datasets\\\\alfred\\\\data\\\\ALFRED_json_2.1.0\\\\'\n",
    "LAPTOP_ALFRED_DATA = '/Users/riordan/Desktop/datasets/alfred/data/ALFRED_json_2.1.0/' #Using the json data because image features arent used yet...\n",
    "JSON_PATH = '/*/*/*.json'\n",
    "if COMPUTER == 'PC':\n",
    "    JSON_PATH = JSON_PATH.replace('/','\\\\')\n",
    "TEST_JSON_PATH = '/*/*.json'\n",
    "if COMPUTER == 'PC':\n",
    "    TEST_JSON_PATH = TEST_JSON_PATH.replace('/','\\\\')\n",
    "ALFRED_TRAIN_INSTRUCTION_TSV_FILENAME = 'ALFRED_Train_InstActionArgs.tsv'\n",
    "ALFRED_VALIDATION_INSTRUCTION_TSV_FILENAME = 'ALFRED_Validation_InstActionArgs.tsv'\n",
    "ALFRED_TEST_INSTRUCTION_TSV_FILENAME = 'ALFRED_Test_InstActionArgs.tsv'\n",
    "\n",
    "def load_next_alfred_data(alfred_json_folder):\n",
    "    # Get list of all instructions and their trajectories\n",
    "    # glob.glob gets all files and stores them. iglob makes an iterator.\n",
    "    ALFRED_PATH = (PC_ALFRED_DATA if COMPUTER == 'PC' else LAPTOP_ALFRED_DATA) + alfred_json_folder\n",
    "    train_json_files = glob.glob(ALFRED_PATH + JSON_PATH)\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    dataset = []\n",
    "    \n",
    "    # Yeild an alfred json\n",
    "    for json_file_idx in tqdm.tqdm(range(len(train_json_files))):\n",
    "        data = json.load(open(train_json_files[json_file_idx]))\n",
    "        instructions = data['turk_annotations']['anns']\n",
    "        actions = data['plan']['high_pddl']\n",
    "        scene = data['scene']\n",
    "        \n",
    "        instruction_actions = []\n",
    "        for d in instructions:\n",
    "            trajectory = {'task_num': 0, 'task_desc': [], 'instructions': []}\n",
    "            trajectory['task_num'] = json_file_idx\n",
    "            trajectory['task_desc'] = d['task_desc']\n",
    "            for i in range(len(d['high_descs'])):\n",
    "                instruction = {'instruction': tokenizer(d['high_descs'][i]), 'action': actions[i]['discrete_action']['action'],\n",
    "                               'argument_1': actions[i]['discrete_action']['args'][0] if 0 < len(actions[i]['discrete_action']['args']) else '<unk>', \n",
    "                               'argument_2': actions[i]['discrete_action']['args'][1] if 1 < len(actions[i]['discrete_action']['args']) else '<unk>'}\n",
    "                trajectory['instructions'].append(instruction)\n",
    "            instruction_actions.append(trajectory)\n",
    "\n",
    "        dataset.append((instruction_actions, scene))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = load_next_alfred_data('train')\n",
    "print('%3f kb' % (sys.getsizeof(train_dataset) / 1024.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset\n",
    "import random\n",
    "\n",
    "def filtered_dataset_copy(alfred_data, scene):\n",
    "    data_copy = alfred_data.copy()\n",
    "\n",
    "    def filter_scene(x):\n",
    "        return x[1]['floor_plan'] == scene\n",
    "\n",
    "    data_copy = list(filter(filter_scene, data_copy))      \n",
    "\n",
    "    return data_copy\n",
    "\n",
    "dataset25 = filtered_dataset_copy(train_dataset, 'FloorPlan25')\n",
    "# A little random sample for testing\n",
    "print('dataset samples: ', len(dataset25))\n",
    "print(dataset25[0][0])\n",
    "for i in range(3):\n",
    "    print('*' * 20)\n",
    "    example = random.choice(dataset25)\n",
    "    print(example[1]['floor_plan'])\n",
    "    print(example[1]['object_poses'])\n",
    "    for j in example[0]:\n",
    "        print('-' * 20)\n",
    "        print(j['instructions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Dot Test\n",
    "Use the dot product between words and semantic graph. \n",
    "\n",
    "TODO:\n",
    "* Use cosine similarity for scoring, instead of dot product. [DONE]\n",
    "* Normalise affordance features after adding parent representions. [DONE, Note: I averaged affordance features by dividing by 2.0]\n",
    "\n",
    "* Remove puncuation from the instructions.\n",
    "* Instead of adding object features to affordance features score them seperately on nouns and verbs respectively then sum them together for a final score. Disregard determiner words and puncuation.\n",
    "* Add a distance scaling factor around the robot_node after the first instruction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "class DotNet(torch.nn.Module):\n",
    "    def __init__(self, one_hot=False):\n",
    "        super(DotNet, self).__init__()\n",
    "        self.one_hot = one_hot\n",
    "        self.OPTION = 1\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        if self.OPTION == 3:\n",
    "            self.GCN = GCNConv(300, 64)\n",
    "        \n",
    "    def forward(self, data, target):\n",
    "        '''\n",
    "        DotNet scores each node based upon the scores it recieves for each word in the instruction.\n",
    "        I have three ideas for how to deal with non-location nodes:\n",
    "        - 1 [Simplest] Ignore them, compute scores (dot product) between each word vector and each node feature vector\n",
    "        then sum all the scores to produce a final score for each node. (This is naive but a good place to start)\n",
    "        - 2 [Hardest] Compute scores for each node feature then add non-location nodes score to parents\n",
    "        - 3 [Weird] Use a GCN to learn how to combine the node features, then compute scores on those features.\n",
    "        Because this approach has some learning it isnt comparable to the last two but IS comparable to niko's target\n",
    "        net. It'll be proving the efficacy of the linear layers that combine target and node features. \n",
    "        '''\n",
    "        x, edge_index = data.x, data.edge_index      \n",
    "        \n",
    "        if self.one_hot:\n",
    "            # Convert indices to one-hot encoding\n",
    "            max_vector = torch.argmax(torch.cat([x,target]))\n",
    "            num_vectors = x.shape[0]\n",
    "            x_one_hot = torch.zeros(num_vectors, max_vector)\n",
    "            x_one_hot[torch.arange(num_vectors), x.flatten()] = 1.0\n",
    "            \n",
    "            target = target.long()\n",
    "            num_vectors = target.shape[0]\n",
    "            target_one_hot = torch.zeros(num_vectors, max_vector)\n",
    "            target_one_hot[torch.arange(num_vectors), target.flatten()] = 1.0\n",
    "        \n",
    "        if self.OPTION == 1:\n",
    "            scores = torch.zeros(x.shape[0], dtype=torch.float32)\n",
    "            for word in target:\n",
    "                # Cosine sim\n",
    "                # word/node repr dim = 300\n",
    "                # nodes = n\n",
    "                # words = w\n",
    "                word = word.expand_as(x)\n",
    "                scores += self.cos(x, word)\n",
    "            x = scores.view(-1,1)\n",
    "        elif self.OPTION == 2:\n",
    "            pass\n",
    "        elif self.OPTION == 3:\n",
    "            pass\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "m = DotNet()\n",
    "class T:\n",
    "    x = torch.randn(4,10)\n",
    "    edge_index = torch.randn(2, 4)\n",
    "\n",
    "t = T()\n",
    "m(t, torch.randn(10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai2thor\n",
    "import ai2thor.controller\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchtext\n",
    "\n",
    "import pandas as pd\n",
    "import json, pickle, time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "print(ai2thor.__version__)\n",
    "# word_vec = torchtext.vocab.FastText()\n",
    "# controller = ai2thor.controller.Controller(scene='FloorPlan29', grid_size=0.25, visibilityDistance=0.75, quality='Very Low', headless=False)   \n",
    "controller = ai2thor.controller.Controller() #dict(scene='FloorPlan25', grid_size=0.25, visibilityDistance=0.75)) #, quality='Very Low', headless=True))\n",
    "controller.start()\n",
    "controller.step(dict(action='Initialize', headless=True, visibilityDistance=3.0))\n",
    "print('Passing...')\n",
    "event = controller.step(dict(action='Pass'))\n",
    "print('Resetting...')\n",
    "controller.reset('FloorPlan25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run scene_graph.ipynb\n",
    "%run thorEnvironment.ipynb\n",
    "\n",
    "import os\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# Utility Functions\n",
    "\n",
    "def draw_graph(graph):\n",
    "    plt.figure()\n",
    "    x = []\n",
    "    y = []\n",
    "    for node in graph.nodes(data=True):\n",
    "        index = node[0]\n",
    "        data = node[1]\n",
    "        if data['node_type'] == 'object':\n",
    "            name = data['data']['name']\n",
    "            pos = data['data']['position']\n",
    "            x.append(pos['x'])\n",
    "            y.append(pos['z'])\n",
    "    ax = plt.scatter(x, y)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Environment Graph Map')\n",
    "\n",
    "def add_object_features(graph):\n",
    "    \"\"\"\n",
    "    Graph preprocessing step to add object features to their affordance features.\n",
    "    Without this step an affordance selection task will be unaware of objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    class graph_t:\n",
    "        x = graph._torch_graph.x.clone()\n",
    "        edge_index = None\n",
    "    \n",
    "    g = graph_t()\n",
    "    # For each object in a graph, add its features to its affordance\n",
    "    for idx in graph.nodes:\n",
    "        affordances = graph.get_affordances(idx)\n",
    "        affordance_count = len(affordances)\n",
    "        if affordance_count > 0:\n",
    "            for affordance in affordances:\n",
    "                g.x[affordance] += g.x[idx]\n",
    "                g.x[affordance] /= 2.0\n",
    "    return g\n",
    "\n",
    "def thor_restore(init_action, object_poses, object_toggles, dirty_and_empty):\n",
    "    \"\"\"\n",
    "    Restore the Thor simulator to an ALFRED defined state\n",
    "    \"\"\"    \n",
    "    \n",
    "    if len(object_toggles) > 0:\n",
    "        controller.step((dict(action='SetObjectToggles', objectToggles=object_toggles)))\n",
    "    \n",
    "    if dirty_and_empty:\n",
    "        controller.step(dict(action='SetStateOfAllObjects',\n",
    "                           StateChange=\"CanBeDirty\",\n",
    "                           forceAction=True))\n",
    "        controller.step(dict(action='SetStateOfAllObjects',\n",
    "                           StateChange=\"CanBeFilled\",\n",
    "                           forceAction=False))\n",
    "    \n",
    "    controller.step((dict(action='SetObjectPoses', objectPoses=object_poses)))\n",
    "    controller.step(init_action)\n",
    "\n",
    "def valid_target(p_env, target):\n",
    "    \"\"\"\n",
    "    Sanity Check. Skip impossible graphs, requiring multiple steps, or faulty exploration.\n",
    "    It appears that the SceneGraph.find() method cannot replace this functionality.\n",
    "    \"\"\"\n",
    "    in_sim = False\n",
    "    objects = p_env.controller.step(dict(action='Pass')).metadata['objects']\n",
    "    for obj in objects:\n",
    "        if target == obj['objectType'].lower():\n",
    "            in_sim = True\n",
    "    \n",
    "    in_graph = False\n",
    "    for n in env.graph.nodes:\n",
    "        if env.graph.nodes[n]['node_type'] == 'object':\n",
    "            if env.graph.nodes[n]['obj'] == target:\n",
    "                in_graph = True\n",
    "    \n",
    "    if not in_sim and not in_graph:\n",
    "        print('[CHECK][MAJOR ERROR] - Target not found in simulation or graphmap')\n",
    "        return False\n",
    "    elif not in_sim and in_graph:\n",
    "        print('[CHECK][MAJOR ERROR] - Target [%s] found in graphmap but not simulation. Perhaps a trajectory/sim mismatch?' % target)\n",
    "        return False\n",
    "    elif in_sim and not in_graph:\n",
    "        print('[CHECK][NOTE] - Target found in sim but not added to graphmap due to not being found in exploration.')\n",
    "        return True\n",
    "    elif in_sim and in_graph:\n",
    "        return True\n",
    "\n",
    "def valid_action(p_env, action):\n",
    "    \"\"\"\n",
    "    In future I'd like all actions/affordances to be supported. I am currently working on this.\n",
    "    This function is used to invalidate trajectories, this reduces available data and is bad. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        normalize_action_name(action)\n",
    "        return True\n",
    "    except NotImplementedError:\n",
    "        return False\n",
    "\n",
    "def valid_trajectory(p_env, trajectory):\n",
    "    \"\"\"\n",
    "    Sanity Check. Goes through each instuction and checks that the targets exist\n",
    "    and its actions can be performed. WARNING: An object meant to be discovered inside an object\n",
    "    will not be detected, TODO: Do Check thor Environment for things contains inside things.\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    for inst_idx, instruction in enumerate(trajectory['instructions']):\n",
    "        target_object = instruction['argument_1']\n",
    "        target_action = instruction['action']\n",
    "        if not valid_target(p_env, target_object):\n",
    "            print('\\t[CHECK] Invalid Instruction \\\"%s\\\" Target Not Found: [%s]' % (' '.join(instruction['instruction']), target_object))\n",
    "            valid = False\n",
    "            break\n",
    "        if not valid_action(p_env, target_action):\n",
    "            print('\\t[CHECK] Invalid action/affordance \\\" %s \\\" Is not currently supported!' % target_action)\n",
    "            valid = False\n",
    "            break\n",
    "        \n",
    "    return valid\n",
    "\n",
    "def print_logit_scores(graph, logits):\n",
    "    \"\"\"\n",
    "    A helpful debug function for viewing the dot product scores for each node in the graph.\n",
    "    \"\"\"\n",
    "    dot_values = {}\n",
    "    \n",
    "    for k,n in graph.nodes.items():\n",
    "        name = ''\n",
    "        if n['node_type'] == 'affordance':\n",
    "            name = n['affordance']\n",
    "        elif n['node_type'] == 'object':\n",
    "            name = n['obj']\n",
    "        dot_values[k] = name, logits[0][k]\n",
    "\n",
    "    dot_values = sorted(dot_values.values(), key=lambda x: x[1])\n",
    "    \n",
    "    for i in dot_values:\n",
    "        print(i)\n",
    "\n",
    "def normalize_action_name(name):\n",
    "    if name == 'GotoLocation':\n",
    "        return 'go'\n",
    "    elif name == 'PickupObject':\n",
    "        return 'pick'\n",
    "    elif name == 'PutObject':\n",
    "        return 'put'\n",
    "    elif name == 'OpenObject':\n",
    "        return 'open'\n",
    "    elif name == 'CloseObject':\n",
    "        return 'close'\n",
    "    elif name == 'SliceObject':\n",
    "        return 'slice'\n",
    "    elif name == 'CleanObject':\n",
    "        return 'clean'\n",
    "    #elif name == 'HeatObject':\n",
    "    #    return 'heat'\n",
    "    #elif name == 'CoolObject':\n",
    "    #    return 'cool'\n",
    "    elif name == 'CookObject':\n",
    "        return 'cook'\n",
    "    else:\n",
    "        raise NotImplementedError(\"Action %s not implimented yet.\" % name)\n",
    "\n",
    "# Seed for reproduceability\n",
    "np.random.seed(42)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using: ',device)\n",
    "\n",
    "# Experiment parameters\n",
    "one_hot = False # One hot vector encoding\n",
    "debug_instructions = False\n",
    "debug_scores = False\n",
    "debug_log = True\n",
    "subset = dataset25[:]\n",
    "model = DotNet(one_hot=one_hot).to(device)\n",
    "\n",
    "# The test is for a multiple instructions intended to be executed over multiple timesteps of length 1\n",
    "max_timesteps = 1 # TODO: Consider increasing to increase performance, niko used 10 i think.\n",
    "reward = []\n",
    "used_timesteps = []\n",
    "success = []\n",
    "task_number = 0\n",
    "\n",
    "# Experiment Log\n",
    "if debug_log:\n",
    "    debug_log_file = open('baseline_debug_log.tsv', 'w')\n",
    "    debug_log_file.write(\"scene \\t instruction \\t expected_object \\t expected_affordance \\t actual_object \\t actual_affordance \\t success\\n\")\n",
    "\n",
    "\n",
    "# run for each instruction in a task\n",
    "for task in subset:\n",
    "    trajectories, scene = task\n",
    "    \n",
    "    scene_name = \"FloorPlan%d\" % scene['scene_num']\n",
    "    object_poses = scene['object_poses']\n",
    "    object_toggles = scene['object_toggles']\n",
    "    dirty_and_empty = scene['dirty_and_empty']\n",
    "    init_action = scene['init_action']\n",
    "    \n",
    "    # ALFRED has a custom initialised scene for each task\n",
    "    # This is expensive, every task requires new initalisations and therefore new explorations\n",
    "    environment_file = \"saved_environments/{}_{}.pickle\".format(scene_name, hash(str(object_poses)))\n",
    "    \n",
    "    episode = {'log_probs':[], 'rewards':[], 'timesteps':0, 'entropy':[]}\n",
    "    done = False\n",
    "    t = 0\n",
    "    task_number += 1\n",
    "    print(\" --- %d / %d --- \" % (task_number, len(subset)))\n",
    "    \n",
    "    for traj_idx, traj in enumerate(trajectories):\n",
    "        # Each trajectory in a task uses the same environment but must be reset\n",
    "        # each trajectory.\n",
    "        controller.reset(scene_name)\n",
    "        env = ThorEnvironment(controller=controller)\n",
    "        \n",
    "        if os.path.isfile(environment_file):\n",
    "            # load file, if exists\n",
    "            thor_restore(init_action, object_poses, object_toggles, dirty_and_empty)\n",
    "            print('Loading environment...')\n",
    "            env.graph = env.graph.from_pickle(environment_file)\n",
    "            if not hasattr(env.graph, 'robot_node'):\n",
    "                env.graph.setup()\n",
    "            print('Done')\n",
    "        else:\n",
    "            # explore and save\n",
    "            thor_restore(init_action, object_poses, object_toggles, dirty_and_empty)\n",
    "            print('Exploring environment...')\n",
    "            env.explore_environment()\n",
    "            env.graph.setup()\n",
    "            print('Saving environment...')\n",
    "            env.graph.to_pickle(environment_file)\n",
    "            print('Done')\n",
    "        \n",
    "        ###########################################################\n",
    "        # The following continue statement is here to make sure all\n",
    "        # we are doing is exploring environments for caching.\n",
    "        # Exploring and then using those graphs could (should) cause\n",
    "        # this script to crash right now. So instead ill just explore\n",
    "        # then once that is done I will fix the crash code where we \n",
    "        # use the graph. Since the graphs will be cached testing\n",
    "        # will be quicker!\n",
    "        ###########################################################\n",
    "        #continue\n",
    "        #draw_graph(env.graph)\n",
    "        #assert False\n",
    "        \n",
    "        complete_trajectory = True\n",
    "        \n",
    "        if not valid_trajectory(env, traj):\n",
    "            print('[CHECK] Invalid trajectory. Skipping...')\n",
    "            continue\n",
    "        else:\n",
    "            print('[CHECK] Valid Trajectory!')\n",
    "        \n",
    "        if debug_instructions:\n",
    "            print(\"Task Trajectory: %d / %d (%d instructions)\" % (traj_idx+1, len(trajectories), len(traj['instructions'])))\n",
    "        \n",
    "        for inst_idx, instruction in enumerate(traj['instructions']):\n",
    "            if debug_instructions:\n",
    "                print('-----> Instruction %d / %d' % (inst_idx + 1, len(traj['instructions'])))            \n",
    "            \n",
    "            target_object = instruction['argument_1']\n",
    "            target_affordance = instruction['action']\n",
    "                        \n",
    "            if True:\n",
    "                if debug_instructions:\n",
    "                    print(\"Target affordance: %s -> %s\\nTarget Instruction: %s\" % (instruction['action'], target_object, ' '.join(instruction['instruction'])))\n",
    "                target_embedding = [env.graph.word2vec(word.lower()) for word in instruction['instruction']]\n",
    "            else:\n",
    "                # Sanity check for \"ground truth\" instructions\n",
    "                sanity_check_instruction = ['pick', target_object] # WARNING: If the task affordance is chance so shall the 'pick' verb.\n",
    "                if debug_instructions:\n",
    "                    print(\"Target object: %s\\nTarget Instruction: %s\" % (target_object, ' '.join(sanity_check_instruction)))\n",
    "                target_embedding = [env.graph.word2vec(word.lower()) for word in sanity_check_instruction]\n",
    "            \n",
    "\n",
    "            #done = False\n",
    "            for timestep in range(max_timesteps):\n",
    "                \n",
    "                env.graph.to_torch_graph()\n",
    "\n",
    "                # Experimental, adds the object features to the affordances\n",
    "                object_smooth_graph = add_object_features(env.graph)\n",
    "\n",
    "                # run the policy network\n",
    "                target = torch.tensor(target_embedding, dtype=torch.float).to(device)\n",
    "                #logits = model(env.graph._torch_graph.to(device), target).t()\n",
    "                logits = model(object_smooth_graph, target).t()\n",
    "\n",
    "                # we only care about the affordance nodes, so set the logits of all other nodes to something very small\n",
    "                mask = torch.tensor(env.graph._torch_affordance_mask==0, dtype=torch.bool).view(1,-1).to(device)\n",
    "                logits[mask] = -1000\n",
    "\n",
    "                # sample an action from the output of the network\n",
    "                action = torch.argmax(logits)\n",
    "                affordance = env.graph.from_torch_id(action.item())\n",
    "                \n",
    "                correct_affordance = False\n",
    "                actual_affordance = env.graph.nodes[affordance]['affordance']\n",
    "                if actual_affordance == normalize_action_name(target_affordance): # WARNING: When the task verb changes so shall 'pick'\n",
    "                    correct_affordance = True\n",
    "                else:\n",
    "                    # Exit early if affordance fails\n",
    "                    complete_trajectory = False\n",
    "                    break\n",
    "\n",
    "                # Compute reward\n",
    "                r = 0.0\n",
    "                succ, pred = env.graph.get_related_objects(affordance)\n",
    "                \n",
    "                correct_object = False\n",
    "                actual_object = None\n",
    "                \n",
    "                if not pred is None:\n",
    "                    for n in pred:\n",
    "                        actual_object = env.graph.nodes[n]['obj'] \n",
    "                        if actual_object == target_object:\n",
    "                            if correct_affordance:\n",
    "                                r = 1.0\n",
    "                                #done = True\n",
    "                                correct_object = True\n",
    "                                # Rio Note: I've decided that Im only going to perform the action if\n",
    "                                # the affordance on the correct node is found. If not the action can mess up the\n",
    "                                # graph to the point where other actions cant be performed. \n",
    "                                # Consider breaking a task trajectory when a single instruction is missed. \n",
    "                                # Perform action. Note: Out of order instructions will often fail on ai2thor simulator.\n",
    "                                # Therefore, TODO: Break a trajectory loop if even a single instruction is not executed\n",
    "                                # in the permited timesteps\n",
    "                                env.step(action.item()) # An Action is an affordance in the graph\n",
    "                                # TODO: If action cannot be executed, break loop and print error. \n",
    "                        else:\n",
    "                            complete_trajectory = False\n",
    "                            break\n",
    "                    \n",
    "                    print(\"Target: %s -> %s, Actual: %s -> %s [%s]\" % (normalize_action_name(target_affordance), target_object, actual_affordance, actual_object, correct_affordance and correct_object))\n",
    "                \n",
    "                if debug_log:\n",
    "                    debug_log_file.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % (scene_name, ' '.join(instruction['instruction']), target_object, target_affordance, env.graph.nodes[n]['obj'], actual_affordance, correct_object and correct_affordance))\n",
    "                \n",
    "                episode['rewards'].append(r)\n",
    "                episode['timesteps'] = t    \n",
    "        \n",
    "        if complete_trajectory:\n",
    "            success.append(1.0)\n",
    "        else:\n",
    "            success.append(0.0)\n",
    "    \n",
    "    # === end of an episode ===\n",
    "    used_timesteps.append(t)\n",
    "    \n",
    "    reward.append(np.sum(episode['rewards']))\n",
    "\n",
    "if debug_log:\n",
    "    debug_log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise Baseline Performance\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Successes: \", len(success))\n",
    "\n",
    "save_figures = False\n",
    "\n",
    "plt.close()\n",
    "plt.figure()\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(data=reward,palette='Set1')\n",
    "ax.set(xlabel='Timesteps', ylabel='Reward (Higher is better)')\n",
    "plt.title('Environment 25 Pickup Instructions')\n",
    "plt.tight_layout()\n",
    "if save_figures:\n",
    "    plt.savefig('dotnet-environment25-pickup-reward.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.lineplot(data=success,palette='Set1')\n",
    "ax.set(xlabel='Instructions', ylabel='Success')\n",
    "plt.title('Environment 25 - Pickup Instructions')\n",
    "plt.tight_layout()\n",
    "if save_figures:\n",
    "    plt.savefig('dotnet-environment25-pickup-success.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie([sum(success), len(success) - sum(success)], explode=(0.0,0.1), labels=('Success', 'Failure'), autopct='%1.1f%%',\n",
    "        shadow=False, startangle=45)\n",
    "ax.axis('equal')\n",
    "plt.title('Environment 25 - Pickup Task Success Rate (%d Instructions)' % len(success))\n",
    "if save_figures:\n",
    "    plt.savefig('dotnet-environment25-full_trajectories-pickup-success-rate.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai2thor.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(100).view(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import pathlib\n",
    "\n",
    "#=========================================================\n",
    "#import torchtext\n",
    "\n",
    "# Load FastText word vectors\n",
    "#fast_text = torchtext.vocab.FastText(cache='../.vector_cache')\n",
    "\n",
    "# Load ALFRED training data using torchtext\n",
    "import torchtext.data as data\n",
    "\n",
    "#  Basic english normalisation, lowers and seperates grammar\n",
    "INSTRUCTION = data.Field(tokenize='basic_english', lower=True,\n",
    "                         init_token='<sos>',\n",
    "                         eos_token='<eos>',)\n",
    "ACTION = data.Field(is_target=True)\n",
    "ACTION_ARGUMENT_1 = data.Field(tokenize='basic_english',is_target=True)\n",
    "ACTION_ARGUMENT_2 = data.Field(tokenize='basic_english',is_target=True)\n",
    "\n",
    "ALFRED_DATA_ROOT = '../honours_research_repo/'\n",
    "ALFRED_TRAIN_INSTRUCTION_TSV_FILENAME = ALFRED_DATA_ROOT + 'ALFRED_Train_InstActionArgs.tsv'\n",
    "ALFRED_VALIDATION_INSTRUCTION_TSV_FILENAME = ALFRED_DATA_ROOT + 'ALFRED_Validation_InstActionArgs.tsv'\n",
    "ALFRED_TEST_INSTRUCTION_TSV_FILENAME = ALFRED_DATA_ROOT + 'ALFRED_Test_InstActionArgs.tsv'\n",
    "\n",
    "def filter_action(example):\n",
    "    if example.action[0] == 'PickupObject':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "    path='', \n",
    "    train=ALFRED_TRAIN_INSTRUCTION_TSV_FILENAME,\n",
    "    validation=ALFRED_VALIDATION_INSTRUCTION_TSV_FILENAME,\n",
    "    test=ALFRED_TEST_INSTRUCTION_TSV_FILENAME, format='tsv',\n",
    "    fields=[('trajectory_number', None),\n",
    "            ('task_description', None),\n",
    "            ('instruction_number', None),\n",
    "            ('instruction', INSTRUCTION),\n",
    "            ('action', ACTION),\n",
    "            ('action_argument_1', ACTION_ARGUMENT_1),\n",
    "            ('action_argument_2', ACTION_ARGUMENT_2)\n",
    "           ], \n",
    "    filter_pred=filter_action)\n",
    "\n",
    "INSTRUCTION.build_vocab(train,vectors=__word_vec__) #'fasttext.en.300d')\n",
    "ACTION.build_vocab(train)\n",
    "ACTION_ARGUMENT_1.build_vocab(train,vectors=__word_vec__)\n",
    "ACTION_ARGUMENT_2.build_vocab(train,vectors=__word_vec__)\n",
    "#========================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
